{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LDA Model\n",
    "=========\n",
    "\n",
    "Introduces Gensim's LDA model and demonstrates its use on the NIPS corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to demonstrate training an LDA model and\n",
    "obtaining good results.\n",
    "\n",
    "In this tutorial we will:\n",
    "\n",
    "* Load data.\n",
    "* Pre-process data.\n",
    "* Transform documents to a vectorized form.\n",
    "* Train an LDA model.\n",
    "\n",
    "This tutorial will **not**:\n",
    "\n",
    "* Explain how Latent Dirichlet Allocation works\n",
    "* Explain how the LDA model performs inference\n",
    "* Teach you how to use Gensim's LDA implementation in its entirety\n",
    "\n",
    "If you are not familiar with the LDA model or how to use it in Gensim, I\n",
    "suggest you read up on that before continuing with this tutorial. Basic\n",
    "understanding of the LDA model should suffice. Examples:\n",
    "\n",
    "* `Introduction to Latent Dirichlet Allocation <http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation>`_\n",
    "* Gensim tutorial: `sphx_glr_auto_examples_core_run_topics_and_transformations.py`\n",
    "* Gensim's LDA model API docs: :py:class:`gensim.models.LdaModel`\n",
    "\n",
    "I would also encourage you to consider each step when applying the model to\n",
    "your data, instead of just blindly applying my solution. The different steps\n",
    "will depend on your data and possibly your goal with the model.\n",
    "\n",
    "Data\n",
    "----\n",
    "\n",
    "I have used a corpus of NIPS papers in this tutorial, but if you're following\n",
    "this tutorial just to learn about LDA I encourage you to consider picking a\n",
    "corpus on a subject that you are familiar with. Qualitatively evaluating the\n",
    "output of an LDA model is challenging and can require you to understand the\n",
    "subject matter of your corpus (depending on your goal with the model).\n",
    "\n",
    "NIPS (Neural Information Processing Systems) is a machine learning conference\n",
    "so the subject matter should be well suited for most of the target audience\n",
    "of this tutorial.  You can download the original data from Sam Roweis'\n",
    "`website <http://www.cs.nyu.edu/~roweis/data.html>`_.  The code below will\n",
    "also do that for you.\n",
    "\n",
    ".. Important::\n",
    "    The corpus contains 1740 documents, and not particularly long ones.\n",
    "    So keep in mind that this tutorial is not geared towards efficiency, and be\n",
    "    careful before applying the code to a large dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import smart_open\n",
    "\n",
    "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "    fname = url.split('/')[-1]\n",
    "    \n",
    "    # Download the file to local storage first.\n",
    "    # We can't read it on the fly because of \n",
    "    # https://github.com/RaRe-Technologies/smart_open/issues/331\n",
    "    if not os.path.isfile(fname):\n",
    "        with smart_open.open(url, \"rb\") as fin:\n",
    "            with smart_open.open(fname, 'wb') as fout:\n",
    "                while True:\n",
    "                    buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n",
    "                    if not buf:\n",
    "                        break\n",
    "                    fout.write(buf)\n",
    "                         \n",
    "    with tarfile.open(fname, mode='r:gz') as tar:\n",
    "        # Ignore directory entries, as well as files like README, etc.\n",
    "        files = [\n",
    "            m for m in tar.getmembers()\n",
    "            if m.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', m.name)\n",
    "        ]\n",
    "        for member in sorted(files, key=lambda x: x.name):\n",
    "            member_bytes = tar.extractfile(member).read()\n",
    "            yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "docs = list(extract_documents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a list of 1740 documents, where each document is a Unicode string. \n",
    "If you're thinking about using your own corpus, then you need to make sure\n",
    "that it's in the same format (list of Unicode strings) before proceeding\n",
    "with the rest of this tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "1 \n",
      "CONNECTIVITY VERSUS ENTROPY \n",
      "Yaser S. Abu-Mostafa \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "ABSTRACT \n",
      "How does the connectivity of a neural network (number of synapses per \n",
      "neuron) relate to the complexity of the problems it can handle (measured by \n",
      "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
      "functions can be implemented using a circuit with very low connectivity (e.g., \n",
      "using two-input NAND gates). However, for a network that learns a pr\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process and vectorize the documents\n",
    "---------------------------------------\n",
    "\n",
    "As part of preprocessing, we will:\n",
    "\n",
    "* Tokenize (split the documents into tokens).\n",
    "* Lemmatize the tokens.\n",
    "* Compute bigrams.\n",
    "* Compute a bag-of-words representation of the data.\n",
    "\n",
    "First we tokenize the text using a regular expression tokenizer from NLTK. We\n",
    "remove numeric tokens and tokens that are only a single character, as they\n",
    "don't tend to be useful, and the dataset contains a lot of them.\n",
    "\n",
    ".. Important::\n",
    "\n",
    "   This tutorial uses the nltk library for preprocessing, although you can\n",
    "   replace it with something else if you want.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the WordNet lemmatizer from NLTK. A lemmatizer is preferred over a\n",
    "stemmer in this case because it produces more readable words. Output that is\n",
    "easy to read is very desirable in topic modelling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the documents.\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find bigrams in the documents. Bigrams are sets of two adjacent words.\n",
    "Using bigrams we can get phrases like \"machine_learning\" in our output\n",
    "(spaces are replaced with underscores); without bigrams we would only get\n",
    "\"machine\" and \"learning\".\n",
    "\n",
    "Note that in the code below, we find bigrams and then add them to the\n",
    "original data, because we would like to keep the words \"machine\" and\n",
    "\"learning\" as well as the bigram \"machine_learning\".\n",
    "\n",
    ".. Important::\n",
    "    Computing n-grams of large dataset can be very computationally\n",
    "    and memory intensive.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-25 21:58:37,227 : INFO : collecting all words and their counts\n",
      "2019-10-25 21:58:37,229 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2019-10-25 21:58:51,751 : INFO : collected 1120198 word types from a corpus of 4629808 words (unigram + bigrams) and 1740 sentences\n",
      "2019-10-25 21:58:51,754 : INFO : using 1120198 counts as vocab in Phrases<0 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'connectivity',\n",
       " u'versus',\n",
       " u'entropy',\n",
       " u'yaser',\n",
       " u'abu',\n",
       " u'mostafa',\n",
       " u'california',\n",
       " u'institute',\n",
       " u'of',\n",
       " u'technology',\n",
       " u'pasadena',\n",
       " u'ca',\n",
       " u'abstract',\n",
       " u'how',\n",
       " u'doe',\n",
       " u'the',\n",
       " u'connectivity',\n",
       " u'of',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'number',\n",
       " u'of',\n",
       " u'synapsis',\n",
       " u'per',\n",
       " u'neuron',\n",
       " u'relate',\n",
       " u'to',\n",
       " u'the',\n",
       " u'complexity',\n",
       " u'of',\n",
       " u'the',\n",
       " u'problem',\n",
       " u'it',\n",
       " u'can',\n",
       " u'handle',\n",
       " u'measured',\n",
       " u'by',\n",
       " u'the',\n",
       " u'entropy',\n",
       " u'switching',\n",
       " u'theory',\n",
       " u'would',\n",
       " u'suggest',\n",
       " u'no',\n",
       " u'relation',\n",
       " u'at',\n",
       " u'all',\n",
       " u'since',\n",
       " u'all',\n",
       " u'boolean',\n",
       " u'function',\n",
       " u'can',\n",
       " u'be',\n",
       " u'implemented',\n",
       " u'using',\n",
       " u'circuit',\n",
       " u'with',\n",
       " u'very',\n",
       " u'low',\n",
       " u'connectivity',\n",
       " u'using',\n",
       " u'two',\n",
       " u'input',\n",
       " u'nand',\n",
       " u'gate',\n",
       " u'however',\n",
       " u'for',\n",
       " u'network',\n",
       " u'that',\n",
       " u'learns',\n",
       " u'problem',\n",
       " u'from',\n",
       " u'example',\n",
       " u'using',\n",
       " u'local',\n",
       " u'learning',\n",
       " u'rule',\n",
       " u'we',\n",
       " u'prove',\n",
       " u'that',\n",
       " u'the',\n",
       " u'entropy',\n",
       " u'of',\n",
       " u'the',\n",
       " u'problem',\n",
       " u'becomes',\n",
       " u'lower',\n",
       " u'bound',\n",
       " u'for',\n",
       " u'the',\n",
       " u'connectivity',\n",
       " u'of',\n",
       " u'the',\n",
       " u'network',\n",
       " u'introduction',\n",
       " u'the',\n",
       " u'most',\n",
       " u'distinguishing',\n",
       " u'feature',\n",
       " u'of',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'is',\n",
       " u'their',\n",
       " u'ability',\n",
       " u'to',\n",
       " u'spon',\n",
       " u'taneously',\n",
       " u'learn',\n",
       " u'the',\n",
       " u'desired',\n",
       " u'function',\n",
       " u'from',\n",
       " u'training',\n",
       " u'sample',\n",
       " u'their',\n",
       " u'ability',\n",
       " u'to',\n",
       " u'program',\n",
       " u'themselves',\n",
       " u'clearly',\n",
       " u'given',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'cannot',\n",
       " u'just',\n",
       " u'learn',\n",
       " u'any',\n",
       " u'function',\n",
       " u'there',\n",
       " u'must',\n",
       " u'be',\n",
       " u'some',\n",
       " u'restriction',\n",
       " u'on',\n",
       " u'which',\n",
       " u'network',\n",
       " u'can',\n",
       " u'learn',\n",
       " u'which',\n",
       " u'function',\n",
       " u'one',\n",
       " u'obvious',\n",
       " u'restriction',\n",
       " u'which',\n",
       " u'is',\n",
       " u'independent',\n",
       " u'of',\n",
       " u'the',\n",
       " u'learning',\n",
       " u'aspect',\n",
       " u'is',\n",
       " u'that',\n",
       " u'the',\n",
       " u'network',\n",
       " u'must',\n",
       " u'be',\n",
       " u'big',\n",
       " u'enough',\n",
       " u'to',\n",
       " u'accommodate',\n",
       " u'the',\n",
       " u'circuit',\n",
       " u'complex',\n",
       " u'ity',\n",
       " u'of',\n",
       " u'the',\n",
       " u'function',\n",
       " u'it',\n",
       " u'will',\n",
       " u'eventually',\n",
       " u'simulate',\n",
       " u'are',\n",
       " u'there',\n",
       " u'restriction',\n",
       " u'that',\n",
       " u'arise',\n",
       " u'merely',\n",
       " u'from',\n",
       " u'the',\n",
       " u'fact',\n",
       " u'that',\n",
       " u'the',\n",
       " u'network',\n",
       " u'is',\n",
       " u'expected',\n",
       " u'to',\n",
       " u'learn',\n",
       " u'the',\n",
       " u'function',\n",
       " u'rather',\n",
       " u'than',\n",
       " u'being',\n",
       " u'purposely',\n",
       " u'designed',\n",
       " u'for',\n",
       " u'the',\n",
       " u'function',\n",
       " u'this',\n",
       " u'paper',\n",
       " u'report',\n",
       " u'restriction',\n",
       " u'of',\n",
       " u'this',\n",
       " u'kind',\n",
       " u'the',\n",
       " u'result',\n",
       " u'imposes',\n",
       " u'lower',\n",
       " u'bound',\n",
       " u'on',\n",
       " u'the',\n",
       " u'connectivity',\n",
       " u'of',\n",
       " u'the',\n",
       " u'network',\n",
       " u'num',\n",
       " u'ber',\n",
       " u'of',\n",
       " u'synapsis',\n",
       " u'per',\n",
       " u'neuron',\n",
       " u'this',\n",
       " u'lower',\n",
       " u'bound',\n",
       " u'can',\n",
       " u'only',\n",
       " u'be',\n",
       " u'consequence',\n",
       " u'of',\n",
       " u'the',\n",
       " u'learning',\n",
       " u'aspect',\n",
       " u'since',\n",
       " u'switching',\n",
       " u'theory',\n",
       " u'provides',\n",
       " u'purposely',\n",
       " u'designed',\n",
       " u'circuit',\n",
       " u'of',\n",
       " u'low',\n",
       " u'connectivity',\n",
       " u'using',\n",
       " u'only',\n",
       " u'two',\n",
       " u'input',\n",
       " u'nand',\n",
       " u'gate',\n",
       " u'capable',\n",
       " u'of',\n",
       " u'imple',\n",
       " u'menting',\n",
       " u'any',\n",
       " u'boolean',\n",
       " u'function',\n",
       " u'it',\n",
       " u'also',\n",
       " u'follows',\n",
       " u'that',\n",
       " u'the',\n",
       " u'learning',\n",
       " u'mechanism',\n",
       " u'must',\n",
       " u'be',\n",
       " u'restricted',\n",
       " u'for',\n",
       " u'this',\n",
       " u'lower',\n",
       " u'bound',\n",
       " u'to',\n",
       " u'hold',\n",
       " u'powerful',\n",
       " u'mechanism',\n",
       " u'can',\n",
       " u'be',\n",
       " u'american',\n",
       " u'institute',\n",
       " u'of',\n",
       " u'physic',\n",
       " u'designed',\n",
       " u'that',\n",
       " u'will',\n",
       " u'find',\n",
       " u'one',\n",
       " u'of',\n",
       " u'the',\n",
       " u'low',\n",
       " u'connectivity',\n",
       " u'circuit',\n",
       " u'perhaps',\n",
       " u'by',\n",
       " u'exhaus',\n",
       " u'tive',\n",
       " u'search',\n",
       " u'and',\n",
       " u'hence',\n",
       " u'the',\n",
       " u'lower',\n",
       " u'bound',\n",
       " u'on',\n",
       " u'connectivity',\n",
       " u'cannot',\n",
       " u'hold',\n",
       " u'in',\n",
       " u'general',\n",
       " u'indeed',\n",
       " u'we',\n",
       " u'restrict',\n",
       " u'the',\n",
       " u'learning',\n",
       " u'mechanism',\n",
       " u'to',\n",
       " u'be',\n",
       " u'local',\n",
       " u'when',\n",
       " u'training',\n",
       " u'sample',\n",
       " u'is',\n",
       " u'loaded',\n",
       " u'into',\n",
       " u'the',\n",
       " u'network',\n",
       " u'each',\n",
       " u'neuron',\n",
       " u'ha',\n",
       " u'access',\n",
       " u'only',\n",
       " u'to',\n",
       " u'those',\n",
       " u'bit',\n",
       " u'carried',\n",
       " u'by',\n",
       " u'itself',\n",
       " u'and',\n",
       " u'the',\n",
       " u'neuron',\n",
       " u'it',\n",
       " u'is',\n",
       " u'directly',\n",
       " u'connected',\n",
       " u'to',\n",
       " u'this',\n",
       " u'is',\n",
       " u'strong',\n",
       " u'assumption',\n",
       " u'that',\n",
       " u'excludes',\n",
       " u'sophisticated',\n",
       " u'learning',\n",
       " u'mechanism',\n",
       " u'used',\n",
       " u'in',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'model',\n",
       " u'but',\n",
       " u'may',\n",
       " u'be',\n",
       " u'more',\n",
       " u'plausible',\n",
       " u'from',\n",
       " u'biological',\n",
       " u'point',\n",
       " u'of',\n",
       " u'view',\n",
       " u'the',\n",
       " u'lower',\n",
       " u'bound',\n",
       " u'on',\n",
       " u'the',\n",
       " u'connectivity',\n",
       " u'of',\n",
       " u'the',\n",
       " u'network',\n",
       " u'is',\n",
       " u'given',\n",
       " u'in',\n",
       " u'term',\n",
       " u'of',\n",
       " u'the',\n",
       " u'entropy',\n",
       " u'of',\n",
       " u'the',\n",
       " u'environment',\n",
       " u'that',\n",
       " u'provides',\n",
       " u'the',\n",
       " u'training',\n",
       " u'sample',\n",
       " u'entropy',\n",
       " u'is',\n",
       " u'quantitative',\n",
       " u'measure',\n",
       " u'of',\n",
       " u'the',\n",
       " u'disorder',\n",
       " u'or',\n",
       " u'randomness',\n",
       " u'in',\n",
       " u'an',\n",
       " u'environment',\n",
       " u'or',\n",
       " u'equiv',\n",
       " u'alently',\n",
       " u'the',\n",
       " u'amount',\n",
       " u'of',\n",
       " u'information',\n",
       " u'needed',\n",
       " u'to',\n",
       " u'specify',\n",
       " u'the',\n",
       " u'environment',\n",
       " u'there',\n",
       " u'are',\n",
       " u'many',\n",
       " u'different',\n",
       " u'way',\n",
       " u'to',\n",
       " u'define',\n",
       " u'entropy',\n",
       " u'and',\n",
       " u'many',\n",
       " u'technical',\n",
       " u'variation',\n",
       " u'of',\n",
       " u'this',\n",
       " u'concept',\n",
       " u'in',\n",
       " u'the',\n",
       " u'next',\n",
       " u'section',\n",
       " u'we',\n",
       " u'shall',\n",
       " u'introduce',\n",
       " u'the',\n",
       " u'formal',\n",
       " u'definition',\n",
       " u'and',\n",
       " u'result',\n",
       " u'but',\n",
       " u'we',\n",
       " u'start',\n",
       " u'here',\n",
       " u'with',\n",
       " u'an',\n",
       " u'informal',\n",
       " u'exposition',\n",
       " u'of',\n",
       " u'the',\n",
       " u'idea',\n",
       " u'involved',\n",
       " u'the',\n",
       " u'environment',\n",
       " u'in',\n",
       " u'our',\n",
       " u'model',\n",
       " u'produce',\n",
       " u'pattern',\n",
       " u'represented',\n",
       " u'by',\n",
       " u'bit',\n",
       " u'xn',\n",
       " u'pixel',\n",
       " u'in',\n",
       " u'the',\n",
       " u'picture',\n",
       " u'of',\n",
       " u'visual',\n",
       " u'scene',\n",
       " u'if',\n",
       " u'you',\n",
       " u'will',\n",
       " u'only',\n",
       " u'different',\n",
       " u'pattern',\n",
       " u'can',\n",
       " u'be',\n",
       " u'generated',\n",
       " u'by',\n",
       " u'given',\n",
       " u'environment',\n",
       " u'where',\n",
       " u'the',\n",
       " u'entropy',\n",
       " u'is',\n",
       " u'essentially',\n",
       " u'log',\n",
       " u'no',\n",
       " u'knowledge',\n",
       " u'is',\n",
       " u'assumed',\n",
       " u'about',\n",
       " u'which',\n",
       " u'pattern',\n",
       " u'the',\n",
       " u'en',\n",
       " u'vironment',\n",
       " u'is',\n",
       " u'likely',\n",
       " u'to',\n",
       " u'generate',\n",
       " u'only',\n",
       " u'that',\n",
       " u'there',\n",
       " u'are',\n",
       " u'of',\n",
       " u'them',\n",
       " u'in',\n",
       " u'the',\n",
       " u'learning',\n",
       " u'process',\n",
       " u'huge',\n",
       " u'number',\n",
       " u'of',\n",
       " u'sample',\n",
       " u'pattern',\n",
       " u'are',\n",
       " u'generated',\n",
       " u'at',\n",
       " u'random',\n",
       " u'from',\n",
       " u'the',\n",
       " u'environment',\n",
       " u'and',\n",
       " u'input',\n",
       " u'to',\n",
       " u'the',\n",
       " u'network',\n",
       " u'one',\n",
       " u'bit',\n",
       " u'per',\n",
       " u'neuron',\n",
       " u'the',\n",
       " u'network',\n",
       " u'us',\n",
       " u'this',\n",
       " u'information',\n",
       " u'to',\n",
       " u'set',\n",
       " u'it',\n",
       " u'internal',\n",
       " u'parameter',\n",
       " u'and',\n",
       " u'gradually',\n",
       " u'tune',\n",
       " u'itself',\n",
       " u'to',\n",
       " u'this',\n",
       " u'particular',\n",
       " u'environment',\n",
       " u'because',\n",
       " u'of',\n",
       " u'the',\n",
       " u'network',\n",
       " u'architecture',\n",
       " u'each',\n",
       " u'neuron',\n",
       " u'know',\n",
       " u'only',\n",
       " u'it',\n",
       " u'own',\n",
       " u'bit',\n",
       " u'and',\n",
       " u'at',\n",
       " u'best',\n",
       " u'the',\n",
       " u'bit',\n",
       " u'of',\n",
       " u'the',\n",
       " u'neuron',\n",
       " u'it',\n",
       " u'is',\n",
       " u'directly',\n",
       " u'connected',\n",
       " u'to',\n",
       " u'by',\n",
       " u'synapse',\n",
       " u'hence',\n",
       " u'the',\n",
       " u'learning',\n",
       " u'rule',\n",
       " u'are',\n",
       " u'local',\n",
       " u'neuron',\n",
       " u'doe',\n",
       " u'not',\n",
       " u'have',\n",
       " u'the',\n",
       " u'benefit',\n",
       " u'of',\n",
       " u'the',\n",
       " u'entire',\n",
       " u'global',\n",
       " u'pattern',\n",
       " u'that',\n",
       " u'is',\n",
       " u'being',\n",
       " u'learned',\n",
       " u'after',\n",
       " u'the',\n",
       " u'learning',\n",
       " u'process',\n",
       " u'ha',\n",
       " u'taken',\n",
       " u'place',\n",
       " u'each',\n",
       " u'neuron',\n",
       " u'is',\n",
       " u'ready',\n",
       " u'to',\n",
       " u'perform',\n",
       " u'function',\n",
       " u'derned',\n",
       " u'by',\n",
       " u'vhat',\n",
       " u'it',\n",
       " u'ha',\n",
       " u'arned',\n",
       " u'the',\n",
       " u'collective',\n",
       " u'interaction',\n",
       " u'of',\n",
       " u'the',\n",
       " u'function',\n",
       " u'of',\n",
       " u'the',\n",
       " u'neuron',\n",
       " u'is',\n",
       " u'what',\n",
       " u'defines',\n",
       " u'the',\n",
       " u'overall',\n",
       " u'function',\n",
       " u'of',\n",
       " u'the',\n",
       " u'network',\n",
       " u'the',\n",
       " u'main',\n",
       " u'result',\n",
       " u'of',\n",
       " u'this',\n",
       " u'paper',\n",
       " u'is',\n",
       " u'that',\n",
       " u'roughly',\n",
       " u'speaking',\n",
       " u'if',\n",
       " u'the',\n",
       " u'connectivity',\n",
       " u'of',\n",
       " u'the',\n",
       " u'network',\n",
       " u'is',\n",
       " u'le',\n",
       " u'than',\n",
       " u'the',\n",
       " u'entropy',\n",
       " u'of',\n",
       " u'the',\n",
       " u'environment',\n",
       " u'the',\n",
       " u'network',\n",
       " u'cannot',\n",
       " u'learn',\n",
       " u'about',\n",
       " u'the',\n",
       " u'environment',\n",
       " u'the',\n",
       " u'idea',\n",
       " u'of',\n",
       " u'the',\n",
       " u'proof',\n",
       " u'is',\n",
       " u'to',\n",
       " u'show',\n",
       " u'that',\n",
       " u'if',\n",
       " u'the',\n",
       " u'connectivity',\n",
       " u'is',\n",
       " u'small',\n",
       " u'the',\n",
       " u'final',\n",
       " u'function',\n",
       " u'of',\n",
       " u'each',\n",
       " u'neuron',\n",
       " u'is',\n",
       " u'independent',\n",
       " u'of',\n",
       " u'the',\n",
       " u'environment',\n",
       " u'and',\n",
       " u'hence',\n",
       " u'to',\n",
       " u'conclude',\n",
       " u'that',\n",
       " u'the',\n",
       " u'overall',\n",
       " u'network',\n",
       " u'ha',\n",
       " u'accumulated',\n",
       " u'no',\n",
       " u'information',\n",
       " u'about',\n",
       " u'the',\n",
       " u'environment',\n",
       " u'it',\n",
       " u'is',\n",
       " u'supposed',\n",
       " u'to',\n",
       " u'learn',\n",
       " u'about',\n",
       " u'formal',\n",
       " u'result',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'is',\n",
       " u'an',\n",
       " u'undirected',\n",
       " u'graph',\n",
       " u'the',\n",
       " u'vertex',\n",
       " u'are',\n",
       " u'the',\n",
       " u'neuron',\n",
       " u'and',\n",
       " u'the',\n",
       " u'edge',\n",
       " u'are',\n",
       " u'the',\n",
       " u'synapsis',\n",
       " u'label',\n",
       " u'the',\n",
       " u'neuron',\n",
       " u'and',\n",
       " u'define',\n",
       " u'_c',\n",
       " u'to',\n",
       " u'be',\n",
       " u'the',\n",
       " u'set',\n",
       " u'of',\n",
       " u'neuron',\n",
       " u'connected',\n",
       " u'by',\n",
       " u'synapse',\n",
       " u'to',\n",
       " u'neuron',\n",
       " u'together',\n",
       " u'with',\n",
       " u'neuron',\n",
       " u'itself',\n",
       " u'an',\n",
       " u'environment',\n",
       " u'is',\n",
       " u'subset',\n",
       " u'__c',\n",
       " u'c0',\n",
       " u'each',\n",
       " u'is',\n",
       " u'sample',\n",
       " u'from',\n",
       " u'the',\n",
       " u'environment',\n",
       " u'during',\n",
       " u'learning',\n",
       " u'zl',\n",
       " u'the',\n",
       " u'bit',\n",
       " u'of',\n",
       " u'are',\n",
       " u'loaded',\n",
       " u'into',\n",
       " u'the',\n",
       " u'neuron',\n",
       " u'respectively',\n",
       " u'consider',\n",
       " u'an',\n",
       " u'arbitrary',\n",
       " u'neuron',\n",
       " u'and',\n",
       " u'telabel',\n",
       " u'everything',\n",
       " u'to',\n",
       " u'make',\n",
       " u'become',\n",
       " u'thus',\n",
       " u'the',\n",
       " u'neuron',\n",
       " u'see',\n",
       " u'the',\n",
       " u'first',\n",
       " u'coordinate',\n",
       " u'of',\n",
       " u'each',\n",
       " u'since',\n",
       " u'our',\n",
       " u'result',\n",
       " u'is',\n",
       " u'asymptotic',\n",
       " u'in',\n",
       " u'we',\n",
       " u'will',\n",
       " u'specify',\n",
       " u'a',\n",
       " u'function',\n",
       " u'of',\n",
       " u'an',\n",
       " u'where',\n",
       " u'satifies',\n",
       " u'limn',\n",
       " u'ao',\n",
       " u'ao',\n",
       " u'since',\n",
       " u'the',\n",
       " u'result',\n",
       " u'is',\n",
       " u'also',\n",
       " u'statistical',\n",
       " u'we',\n",
       " u'will',\n",
       " u'consider',\n",
       " u'the',\n",
       " u'ensemble',\n",
       " u'of',\n",
       " u'environment',\n",
       " u'where',\n",
       " u'on',\n",
       " u'and',\n",
       " u'satifies',\n",
       " u'limn_',\n",
       " u'oo',\n",
       " u'the',\n",
       " u'probability',\n",
       " u'distribution',\n",
       " u'on',\n",
       " u'is',\n",
       " u'uniform',\n",
       " u'any',\n",
       " u'environmen',\n",
       " u'likely',\n",
       " u'occur',\n",
       " u'any',\n",
       " u'her',\n",
       " u'the',\n",
       " u'neuron',\n",
       " u'see',\n",
       " u'only',\n",
       " u'he',\n",
       " u'fir',\n",
       " u'coordinate',\n",
       " u'of',\n",
       " u'each',\n",
       " u'generated',\n",
       " u'by',\n",
       " u'he',\n",
       " u'environment',\n",
       " u'for',\n",
       " u'each',\n",
       " u'we',\n",
       " u'define',\n",
       " u'the',\n",
       " u'function',\n",
       " u'where',\n",
       " u'x6e',\n",
       " u'fork',\n",
       " u'and',\n",
       " u'the',\n",
       " u'normalized',\n",
       " u'version',\n",
       " u'the',\n",
       " u'function',\n",
       " u'describes',\n",
       " u'the',\n",
       " u'relative',\n",
       " u'frequency',\n",
       " u'of',\n",
       " u'occurrence',\n",
       " u'for',\n",
       " u'each',\n",
       " u'of',\n",
       " u'the',\n",
       " u'binary',\n",
       " u'vector',\n",
       " u'a',\n",
       " u'zl',\n",
       " u'jr',\n",
       " u'run',\n",
       " u'through',\n",
       " u'all',\n",
       " u'vector',\n",
       " u'in',\n",
       " u'in',\n",
       " u'other',\n",
       " u'word',\n",
       " u'specifies',\n",
       " u'the',\n",
       " u'projection',\n",
       " u'of',\n",
       " u'a',\n",
       " u'seen',\n",
       " u'by',\n",
       " u'the',\n",
       " u'neuron',\n",
       " u'clearly',\n",
       " u'for',\n",
       " u'all',\n",
       " u'and',\n",
       " u'corresponding',\n",
       " u'to',\n",
       " u'two',\n",
       " u'environment',\n",
       " u'el',\n",
       " u'and',\n",
       " u'e',\n",
       " u'we',\n",
       " u'will',\n",
       " u'have',\n",
       " u'two',\n",
       " u'function',\n",
       " u'vl',\n",
       " u'and',\n",
       " u'bt',\n",
       " u'if',\n",
       " u'is',\n",
       " u'not',\n",
       " u'distinguishable',\n",
       " u'from',\n",
       " u'the',\n",
       " u'neuron',\n",
       " u'cannot',\n",
       " u'tell',\n",
       " u'the',\n",
       " u'difference',\n",
       " u'between',\n",
       " u'ea',\n",
       " u'and',\n",
       " u'e',\n",
       " u'the',\n",
       " u'distinguishability',\n",
       " u'between',\n",
       " u'btl',\n",
       " u'and',\n",
       " u'can',\n",
       " u'be',\n",
       " u'measured',\n",
       " u'by',\n",
       " u'the',\n",
       " u'range',\n",
       " u'of',\n",
       " u'is',\n",
       " u'where',\n",
       " u'corresponds',\n",
       " u'to',\n",
       " u'complete',\n",
       " u'indistinguishability',\n",
       " u'while',\n",
       " u'corresponds',\n",
       " u'to',\n",
       " u'maximum',\n",
       " u'distinguishability',\n",
       " u'we',\n",
       " u'axe',\n",
       " u'now',\n",
       " u'in',\n",
       " u'position',\n",
       " u'to',\n",
       " u'state',\n",
       " u'the',\n",
       " u'main',\n",
       " u'result',\n",
       " u'let',\n",
       " u'and',\n",
       " u'e',\n",
       " u'be',\n",
       " u'independently',\n",
       " u'selected',\n",
       " u'environment',\n",
       " u'from',\n",
       " u'according',\n",
       " u'to',\n",
       " u'the',\n",
       " u'uniform',\n",
       " u'probability',\n",
       " u'distribution',\n",
       " u'vl',\n",
       " u'is',\n",
       " u'now',\n",
       " u'random',\n",
       " u'variable',\n",
       " u'and',\n",
       " u'we',\n",
       " u'are',\n",
       " u'interested',\n",
       " u'in',\n",
       " u'the',\n",
       " u'expected',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove rare words and common words based on their *document frequency*.\n",
    "Below we remove words that appear in less than 20 documents or in more than\n",
    "50% of the documents. Consider trying to remove words only based on their\n",
    "frequency, or maybe combining that with this approach.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-25 22:04:15,145 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-25 22:04:20,267 : INFO : built Dictionary(79429 unique tokens: [u'bshouty', u's2s2s3s4s4s', u'gg9ghhhho0112233', u'homomorphism', u'nordisk']...) from 1740 documents (total 4953968 corpus positions)\n",
      "2019-10-25 22:04:20,606 : INFO : discarding 70785 tokens: [(u'0a', 19), (u'2h', 16), (u'2h2', 1), (u'2he', 3), (u'__c', 2), (u'_k', 6), (u'a', 1740), (u'about', 1058), (u'abstract', 1740), (u'after', 1087)]...\n",
      "2019-10-25 22:04:20,608 : INFO : keeping 8644 tokens which were in no less than 20 and no more than 870 (=50.0%) documents\n",
      "2019-10-25 22:04:20,662 : INFO : resulting dictionary: Dictionary(8644 unique tokens: [u'ma_abstract', u'func', u'four', u'circuitry', u'ac_uk']...)\n"
     ]
    }
   ],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform the documents to a vectorized form. We simply compute\n",
    "the frequency of each word, including the bigrams.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 4),\n",
       "  (1, 2),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 2),\n",
       "  (5, 4),\n",
       "  (6, 4),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 2),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 8),\n",
       "  (29, 1),\n",
       "  (30, 3),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 2),\n",
       "  (37, 4),\n",
       "  (38, 2),\n",
       "  (39, 3),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 1),\n",
       "  (52, 3),\n",
       "  (53, 1),\n",
       "  (54, 11),\n",
       "  (55, 1),\n",
       "  (56, 2),\n",
       "  (57, 2),\n",
       "  (58, 12),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 1),\n",
       "  (62, 1),\n",
       "  (63, 1),\n",
       "  (64, 4),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 4),\n",
       "  (71, 2),\n",
       "  (72, 1),\n",
       "  (73, 1),\n",
       "  (74, 1),\n",
       "  (75, 1),\n",
       "  (76, 3),\n",
       "  (77, 2),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 1),\n",
       "  (82, 3),\n",
       "  (83, 12),\n",
       "  (84, 1),\n",
       "  (85, 3),\n",
       "  (86, 1),\n",
       "  (87, 2),\n",
       "  (88, 4),\n",
       "  (89, 4),\n",
       "  (90, 3),\n",
       "  (91, 1),\n",
       "  (92, 1),\n",
       "  (93, 1),\n",
       "  (94, 2),\n",
       "  (95, 1),\n",
       "  (96, 1),\n",
       "  (97, 1),\n",
       "  (98, 1),\n",
       "  (99, 1),\n",
       "  (100, 1),\n",
       "  (101, 1),\n",
       "  (102, 3),\n",
       "  (103, 1),\n",
       "  (104, 6),\n",
       "  (105, 1),\n",
       "  (106, 2),\n",
       "  (107, 1),\n",
       "  (108, 1),\n",
       "  (109, 1),\n",
       "  (110, 1),\n",
       "  (111, 1),\n",
       "  (112, 1),\n",
       "  (113, 3),\n",
       "  (114, 1),\n",
       "  (115, 1),\n",
       "  (116, 1),\n",
       "  (117, 1),\n",
       "  (118, 1),\n",
       "  (119, 1),\n",
       "  (120, 1),\n",
       "  (121, 1),\n",
       "  (122, 1),\n",
       "  (123, 3),\n",
       "  (124, 1),\n",
       "  (125, 9),\n",
       "  (126, 27),\n",
       "  (127, 1),\n",
       "  (128, 1),\n",
       "  (129, 2),\n",
       "  (130, 4),\n",
       "  (131, 2),\n",
       "  (132, 1),\n",
       "  (133, 1),\n",
       "  (134, 1),\n",
       "  (135, 5),\n",
       "  (136, 4),\n",
       "  (137, 1),\n",
       "  (138, 2),\n",
       "  (139, 1),\n",
       "  (140, 1),\n",
       "  (141, 3),\n",
       "  (142, 3),\n",
       "  (143, 1),\n",
       "  (144, 1),\n",
       "  (145, 1),\n",
       "  (146, 1),\n",
       "  (147, 7),\n",
       "  (148, 1),\n",
       "  (149, 2),\n",
       "  (150, 1),\n",
       "  (151, 1),\n",
       "  (152, 2),\n",
       "  (153, 1),\n",
       "  (154, 3),\n",
       "  (155, 2),\n",
       "  (156, 1),\n",
       "  (157, 3),\n",
       "  (158, 2),\n",
       "  (159, 2),\n",
       "  (160, 6),\n",
       "  (161, 2),\n",
       "  (162, 1),\n",
       "  (163, 1),\n",
       "  (164, 1),\n",
       "  (165, 1),\n",
       "  (166, 18),\n",
       "  (167, 2),\n",
       "  (168, 1),\n",
       "  (169, 2),\n",
       "  (170, 10),\n",
       "  (171, 1),\n",
       "  (172, 2),\n",
       "  (173, 1),\n",
       "  (174, 3),\n",
       "  (175, 1),\n",
       "  (176, 3),\n",
       "  (177, 1),\n",
       "  (178, 1),\n",
       "  (179, 1),\n",
       "  (180, 1),\n",
       "  (181, 1),\n",
       "  (182, 1),\n",
       "  (183, 1),\n",
       "  (184, 4),\n",
       "  (185, 1),\n",
       "  (186, 2),\n",
       "  (187, 1),\n",
       "  (188, 2),\n",
       "  (189, 2),\n",
       "  (190, 1),\n",
       "  (191, 1),\n",
       "  (192, 1),\n",
       "  (193, 1),\n",
       "  (194, 1),\n",
       "  (195, 1),\n",
       "  (196, 3),\n",
       "  (197, 1),\n",
       "  (198, 2),\n",
       "  (199, 1),\n",
       "  (200, 1),\n",
       "  (201, 1),\n",
       "  (202, 1),\n",
       "  (203, 1),\n",
       "  (204, 1),\n",
       "  (205, 1),\n",
       "  (206, 1),\n",
       "  (207, 2),\n",
       "  (208, 1),\n",
       "  (209, 6),\n",
       "  (210, 1),\n",
       "  (211, 2),\n",
       "  (212, 1),\n",
       "  (213, 2),\n",
       "  (214, 2),\n",
       "  (215, 1),\n",
       "  (216, 1),\n",
       "  (217, 2),\n",
       "  (218, 4),\n",
       "  (219, 1),\n",
       "  (220, 2),\n",
       "  (221, 1),\n",
       "  (222, 3),\n",
       "  (223, 9),\n",
       "  (224, 9),\n",
       "  (225, 3),\n",
       "  (226, 2),\n",
       "  (227, 2),\n",
       "  (228, 1),\n",
       "  (229, 1),\n",
       "  (230, 1),\n",
       "  (231, 1),\n",
       "  (232, 2),\n",
       "  (233, 4),\n",
       "  (234, 1),\n",
       "  (235, 4),\n",
       "  (236, 1),\n",
       "  (237, 5),\n",
       "  (238, 11),\n",
       "  (239, 1),\n",
       "  (240, 1),\n",
       "  (241, 1),\n",
       "  (242, 27),\n",
       "  (243, 1),\n",
       "  (244, 1),\n",
       "  (245, 4),\n",
       "  (246, 1),\n",
       "  (247, 1),\n",
       "  (248, 1),\n",
       "  (249, 1),\n",
       "  (250, 1),\n",
       "  (251, 1),\n",
       "  (252, 1),\n",
       "  (253, 1),\n",
       "  (254, 1),\n",
       "  (255, 1),\n",
       "  (256, 1),\n",
       "  (257, 3),\n",
       "  (258, 3),\n",
       "  (259, 1),\n",
       "  (260, 1),\n",
       "  (261, 2),\n",
       "  (262, 1),\n",
       "  (263, 1),\n",
       "  (264, 1),\n",
       "  (265, 2),\n",
       "  (266, 1),\n",
       "  (267, 1),\n",
       "  (268, 3),\n",
       "  (269, 1),\n",
       "  (270, 1),\n",
       "  (271, 1),\n",
       "  (272, 2),\n",
       "  (273, 1),\n",
       "  (274, 1),\n",
       "  (275, 1),\n",
       "  (276, 1),\n",
       "  (277, 1),\n",
       "  (278, 2),\n",
       "  (279, 19),\n",
       "  (280, 1),\n",
       "  (281, 1),\n",
       "  (282, 1),\n",
       "  (283, 1),\n",
       "  (284, 4),\n",
       "  (285, 1),\n",
       "  (286, 1),\n",
       "  (287, 1),\n",
       "  (288, 3),\n",
       "  (289, 4),\n",
       "  (290, 2),\n",
       "  (291, 2),\n",
       "  (292, 1),\n",
       "  (293, 1),\n",
       "  (294, 1),\n",
       "  (295, 2),\n",
       "  (296, 1),\n",
       "  (297, 1),\n",
       "  (298, 1),\n",
       "  (299, 1),\n",
       "  (300, 1),\n",
       "  (301, 1),\n",
       "  (302, 1),\n",
       "  (303, 1),\n",
       "  (304, 1),\n",
       "  (305, 1),\n",
       "  (306, 1),\n",
       "  (307, 1),\n",
       "  (308, 1),\n",
       "  (309, 1),\n",
       "  (310, 1),\n",
       "  (311, 1),\n",
       "  (312, 2),\n",
       "  (313, 1),\n",
       "  (314, 1),\n",
       "  (315, 4),\n",
       "  (316, 1),\n",
       "  (317, 1),\n",
       "  (318, 1),\n",
       "  (319, 2),\n",
       "  (320, 1),\n",
       "  (321, 5),\n",
       "  (322, 1),\n",
       "  (323, 1),\n",
       "  (324, 1),\n",
       "  (325, 1),\n",
       "  (326, 1),\n",
       "  (327, 2),\n",
       "  (328, 1),\n",
       "  (329, 1),\n",
       "  (330, 1),\n",
       "  (331, 1),\n",
       "  (332, 1),\n",
       "  (333, 1),\n",
       "  (334, 1),\n",
       "  (335, 1),\n",
       "  (336, 2),\n",
       "  (337, 2),\n",
       "  (338, 1),\n",
       "  (339, 3),\n",
       "  (340, 1),\n",
       "  (341, 1),\n",
       "  (342, 1),\n",
       "  (343, 1),\n",
       "  (344, 1),\n",
       "  (345, 1),\n",
       "  (346, 2),\n",
       "  (347, 1),\n",
       "  (348, 2),\n",
       "  (349, 1),\n",
       "  (350, 1),\n",
       "  (351, 1),\n",
       "  (352, 1),\n",
       "  (353, 3),\n",
       "  (354, 1),\n",
       "  (355, 1),\n",
       "  (356, 2),\n",
       "  (357, 3),\n",
       "  (358, 1),\n",
       "  (359, 1),\n",
       "  (360, 1),\n",
       "  (361, 1),\n",
       "  (362, 1),\n",
       "  (363, 1),\n",
       "  (364, 1),\n",
       "  (365, 8),\n",
       "  (366, 1),\n",
       "  (367, 2),\n",
       "  (368, 1),\n",
       "  (369, 2),\n",
       "  (370, 1),\n",
       "  (371, 1),\n",
       "  (372, 1),\n",
       "  (373, 1),\n",
       "  (374, 1),\n",
       "  (375, 1),\n",
       "  (376, 5),\n",
       "  (377, 3),\n",
       "  (378, 3),\n",
       "  (379, 1),\n",
       "  (380, 5),\n",
       "  (381, 1),\n",
       "  (382, 1),\n",
       "  (383, 1),\n",
       "  (384, 1),\n",
       "  (385, 1),\n",
       "  (386, 1),\n",
       "  (387, 1),\n",
       "  (388, 1),\n",
       "  (389, 1),\n",
       "  (390, 1),\n",
       "  (391, 1),\n",
       "  (392, 1),\n",
       "  (393, 6),\n",
       "  (394, 1),\n",
       "  (395, 1),\n",
       "  (396, 1),\n",
       "  (397, 1),\n",
       "  (398, 2),\n",
       "  (399, 1),\n",
       "  (400, 1),\n",
       "  (401, 1),\n",
       "  (402, 1),\n",
       "  (403, 1),\n",
       "  (404, 1),\n",
       "  (405, 1),\n",
       "  (406, 1),\n",
       "  (407, 1),\n",
       "  (408, 1),\n",
       "  (409, 1),\n",
       "  (410, 1),\n",
       "  (411, 1),\n",
       "  (412, 1),\n",
       "  (413, 2)],\n",
       " [(4, 1),\n",
       "  (17, 3),\n",
       "  (21, 1),\n",
       "  (22, 3),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (26, 16),\n",
       "  (29, 1),\n",
       "  (33, 1),\n",
       "  (36, 1),\n",
       "  (42, 3),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (52, 3),\n",
       "  (53, 5),\n",
       "  (54, 1),\n",
       "  (60, 1),\n",
       "  (64, 1),\n",
       "  (70, 4),\n",
       "  (72, 2),\n",
       "  (73, 2),\n",
       "  (75, 1),\n",
       "  (77, 3),\n",
       "  (81, 5),\n",
       "  (82, 4),\n",
       "  (83, 2),\n",
       "  (90, 1),\n",
       "  (91, 1),\n",
       "  (92, 1),\n",
       "  (93, 3),\n",
       "  (102, 1),\n",
       "  (103, 4),\n",
       "  (104, 1),\n",
       "  (106, 2),\n",
       "  (113, 2),\n",
       "  (116, 1),\n",
       "  (118, 2),\n",
       "  (119, 3),\n",
       "  (128, 1),\n",
       "  (132, 1),\n",
       "  (143, 4),\n",
       "  (150, 1),\n",
       "  (151, 1),\n",
       "  (156, 1),\n",
       "  (157, 2),\n",
       "  (159, 7),\n",
       "  (160, 1),\n",
       "  (170, 1),\n",
       "  (174, 1),\n",
       "  (176, 1),\n",
       "  (181, 5),\n",
       "  (189, 1),\n",
       "  (190, 1),\n",
       "  (192, 1),\n",
       "  (201, 1),\n",
       "  (207, 3),\n",
       "  (209, 1),\n",
       "  (211, 2),\n",
       "  (219, 2),\n",
       "  (222, 5),\n",
       "  (223, 1),\n",
       "  (227, 1),\n",
       "  (230, 1),\n",
       "  (231, 2),\n",
       "  (232, 1),\n",
       "  (233, 1),\n",
       "  (241, 1),\n",
       "  (242, 31),\n",
       "  (243, 1),\n",
       "  (246, 1),\n",
       "  (257, 5),\n",
       "  (264, 1),\n",
       "  (265, 1),\n",
       "  (268, 1),\n",
       "  (270, 2),\n",
       "  (271, 5),\n",
       "  (273, 1),\n",
       "  (274, 1),\n",
       "  (275, 3),\n",
       "  (276, 2),\n",
       "  (277, 1),\n",
       "  (278, 2),\n",
       "  (279, 2),\n",
       "  (281, 3),\n",
       "  (282, 1),\n",
       "  (284, 1),\n",
       "  (290, 2),\n",
       "  (297, 2),\n",
       "  (298, 1),\n",
       "  (299, 1),\n",
       "  (308, 1),\n",
       "  (312, 1),\n",
       "  (319, 10),\n",
       "  (320, 11),\n",
       "  (321, 1),\n",
       "  (326, 12),\n",
       "  (327, 3),\n",
       "  (328, 1),\n",
       "  (331, 1),\n",
       "  (333, 1),\n",
       "  (340, 2),\n",
       "  (341, 1),\n",
       "  (342, 2),\n",
       "  (348, 5),\n",
       "  (353, 3),\n",
       "  (355, 2),\n",
       "  (356, 27),\n",
       "  (357, 1),\n",
       "  (359, 1),\n",
       "  (360, 6),\n",
       "  (363, 2),\n",
       "  (370, 4),\n",
       "  (376, 2),\n",
       "  (379, 1),\n",
       "  (385, 2),\n",
       "  (386, 1),\n",
       "  (390, 2),\n",
       "  (391, 1),\n",
       "  (393, 1),\n",
       "  (394, 1),\n",
       "  (395, 1),\n",
       "  (414, 1),\n",
       "  (415, 1),\n",
       "  (416, 4),\n",
       "  (417, 1),\n",
       "  (418, 1),\n",
       "  (419, 1),\n",
       "  (420, 1),\n",
       "  (421, 1),\n",
       "  (422, 1),\n",
       "  (423, 1),\n",
       "  (424, 1),\n",
       "  (425, 3),\n",
       "  (426, 13),\n",
       "  (427, 1),\n",
       "  (428, 6),\n",
       "  (429, 2),\n",
       "  (430, 2),\n",
       "  (431, 1),\n",
       "  (432, 4),\n",
       "  (433, 3),\n",
       "  (434, 2),\n",
       "  (435, 2),\n",
       "  (436, 1),\n",
       "  (437, 4),\n",
       "  (438, 1),\n",
       "  (439, 1),\n",
       "  (440, 2),\n",
       "  (441, 2),\n",
       "  (442, 2),\n",
       "  (443, 1),\n",
       "  (444, 1),\n",
       "  (445, 2),\n",
       "  (446, 1),\n",
       "  (447, 1),\n",
       "  (448, 1),\n",
       "  (449, 8),\n",
       "  (450, 1),\n",
       "  (451, 2),\n",
       "  (452, 1),\n",
       "  (453, 1),\n",
       "  (454, 4),\n",
       "  (455, 2),\n",
       "  (456, 2),\n",
       "  (457, 2),\n",
       "  (458, 1),\n",
       "  (459, 3),\n",
       "  (460, 1),\n",
       "  (461, 1),\n",
       "  (462, 1),\n",
       "  (463, 1),\n",
       "  (464, 1),\n",
       "  (465, 1),\n",
       "  (466, 1),\n",
       "  (467, 1),\n",
       "  (468, 1),\n",
       "  (469, 1),\n",
       "  (470, 1),\n",
       "  (471, 1),\n",
       "  (472, 1),\n",
       "  (473, 1),\n",
       "  (474, 1),\n",
       "  (475, 1),\n",
       "  (476, 1),\n",
       "  (477, 3),\n",
       "  (478, 2),\n",
       "  (479, 2),\n",
       "  (480, 1),\n",
       "  (481, 1),\n",
       "  (482, 1),\n",
       "  (483, 2),\n",
       "  (484, 2),\n",
       "  (485, 1),\n",
       "  (486, 2),\n",
       "  (487, 1),\n",
       "  (488, 1),\n",
       "  (489, 1),\n",
       "  (490, 2),\n",
       "  (491, 1),\n",
       "  (492, 2),\n",
       "  (493, 1),\n",
       "  (494, 3),\n",
       "  (495, 1),\n",
       "  (496, 1),\n",
       "  (497, 2),\n",
       "  (498, 5),\n",
       "  (499, 5),\n",
       "  (500, 5),\n",
       "  (501, 4),\n",
       "  (502, 1),\n",
       "  (503, 1),\n",
       "  (504, 2),\n",
       "  (505, 1),\n",
       "  (506, 1),\n",
       "  (507, 3),\n",
       "  (508, 1),\n",
       "  (509, 1),\n",
       "  (510, 1),\n",
       "  (511, 2),\n",
       "  (512, 3),\n",
       "  (513, 1),\n",
       "  (514, 2),\n",
       "  (515, 1),\n",
       "  (516, 1),\n",
       "  (517, 1),\n",
       "  (518, 6),\n",
       "  (519, 8),\n",
       "  (520, 1),\n",
       "  (521, 3),\n",
       "  (522, 2),\n",
       "  (523, 1),\n",
       "  (524, 1),\n",
       "  (525, 1),\n",
       "  (526, 1),\n",
       "  (527, 1),\n",
       "  (528, 6),\n",
       "  (529, 5),\n",
       "  (530, 2),\n",
       "  (531, 1),\n",
       "  (532, 7),\n",
       "  (533, 1),\n",
       "  (534, 1),\n",
       "  (535, 1),\n",
       "  (536, 2),\n",
       "  (537, 2),\n",
       "  (538, 1),\n",
       "  (539, 1),\n",
       "  (540, 1),\n",
       "  (541, 3),\n",
       "  (542, 3),\n",
       "  (543, 1),\n",
       "  (544, 3),\n",
       "  (545, 1),\n",
       "  (546, 1),\n",
       "  (547, 3),\n",
       "  (548, 2),\n",
       "  (549, 2),\n",
       "  (550, 1),\n",
       "  (551, 1),\n",
       "  (552, 11),\n",
       "  (553, 7),\n",
       "  (554, 2),\n",
       "  (555, 1),\n",
       "  (556, 3),\n",
       "  (557, 1),\n",
       "  (558, 1),\n",
       "  (559, 1),\n",
       "  (560, 1),\n",
       "  (561, 3),\n",
       "  (562, 6),\n",
       "  (563, 1),\n",
       "  (564, 5),\n",
       "  (565, 1),\n",
       "  (566, 3),\n",
       "  (567, 1),\n",
       "  (568, 1),\n",
       "  (569, 1),\n",
       "  (570, 2),\n",
       "  (571, 1),\n",
       "  (572, 1),\n",
       "  (573, 5),\n",
       "  (574, 4),\n",
       "  (575, 1),\n",
       "  (576, 1),\n",
       "  (577, 2),\n",
       "  (578, 1),\n",
       "  (579, 1),\n",
       "  (580, 1),\n",
       "  (581, 5),\n",
       "  (582, 1),\n",
       "  (583, 2),\n",
       "  (584, 1),\n",
       "  (585, 8),\n",
       "  (586, 1),\n",
       "  (587, 1),\n",
       "  (588, 1),\n",
       "  (589, 1),\n",
       "  (590, 2),\n",
       "  (591, 1),\n",
       "  (592, 1),\n",
       "  (593, 2),\n",
       "  (594, 1),\n",
       "  (595, 1),\n",
       "  (596, 2),\n",
       "  (597, 1),\n",
       "  (598, 1),\n",
       "  (599, 1),\n",
       "  (600, 1),\n",
       "  (601, 1),\n",
       "  (602, 2),\n",
       "  (603, 1),\n",
       "  (604, 1),\n",
       "  (605, 1),\n",
       "  (606, 7),\n",
       "  (607, 1),\n",
       "  (608, 2),\n",
       "  (609, 1),\n",
       "  (610, 1),\n",
       "  (611, 1),\n",
       "  (612, 2),\n",
       "  (613, 1),\n",
       "  (614, 2),\n",
       "  (615, 3),\n",
       "  (616, 1),\n",
       "  (617, 6),\n",
       "  (618, 1),\n",
       "  (619, 1),\n",
       "  (620, 3),\n",
       "  (621, 1),\n",
       "  (622, 1),\n",
       "  (623, 5),\n",
       "  (624, 5),\n",
       "  (625, 2),\n",
       "  (626, 1),\n",
       "  (627, 2),\n",
       "  (628, 1),\n",
       "  (629, 3),\n",
       "  (630, 1),\n",
       "  (631, 1),\n",
       "  (632, 1),\n",
       "  (633, 1),\n",
       "  (634, 1),\n",
       "  (635, 2),\n",
       "  (636, 1),\n",
       "  (637, 1),\n",
       "  (638, 1),\n",
       "  (639, 1),\n",
       "  (640, 1),\n",
       "  (641, 1),\n",
       "  (642, 1),\n",
       "  (643, 1),\n",
       "  (644, 1),\n",
       "  (645, 2),\n",
       "  (646, 2),\n",
       "  (647, 1),\n",
       "  (648, 1),\n",
       "  (649, 6),\n",
       "  (650, 6),\n",
       "  (651, 2),\n",
       "  (652, 1),\n",
       "  (653, 1),\n",
       "  (654, 1),\n",
       "  (655, 1),\n",
       "  (656, 1),\n",
       "  (657, 1),\n",
       "  (658, 1),\n",
       "  (659, 15),\n",
       "  (660, 1),\n",
       "  (661, 1),\n",
       "  (662, 1),\n",
       "  (663, 3),\n",
       "  (664, 1),\n",
       "  (665, 1),\n",
       "  (666, 1),\n",
       "  (667, 1),\n",
       "  (668, 1),\n",
       "  (669, 5),\n",
       "  (670, 1),\n",
       "  (671, 4),\n",
       "  (672, 1),\n",
       "  (673, 1),\n",
       "  (674, 2),\n",
       "  (675, 1),\n",
       "  (676, 1),\n",
       "  (677, 1),\n",
       "  (678, 1),\n",
       "  (679, 1),\n",
       "  (680, 1),\n",
       "  (681, 2),\n",
       "  (682, 1),\n",
       "  (683, 2),\n",
       "  (684, 1),\n",
       "  (685, 1),\n",
       "  (686, 1),\n",
       "  (687, 1),\n",
       "  (688, 3),\n",
       "  (689, 2),\n",
       "  (690, 2),\n",
       "  (691, 9),\n",
       "  (692, 1),\n",
       "  (693, 20),\n",
       "  (694, 7),\n",
       "  (695, 1),\n",
       "  (696, 1),\n",
       "  (697, 4),\n",
       "  (698, 2),\n",
       "  (699, 1),\n",
       "  (700, 1),\n",
       "  (701, 1),\n",
       "  (702, 1),\n",
       "  (703, 1),\n",
       "  (704, 1),\n",
       "  (705, 1),\n",
       "  (706, 1),\n",
       "  (707, 1),\n",
       "  (708, 1),\n",
       "  (709, 1),\n",
       "  (710, 1),\n",
       "  (711, 14),\n",
       "  (712, 3),\n",
       "  (713, 1),\n",
       "  (714, 1),\n",
       "  (715, 1),\n",
       "  (716, 1),\n",
       "  (717, 1),\n",
       "  (718, 1),\n",
       "  (719, 1),\n",
       "  (720, 1),\n",
       "  (721, 1),\n",
       "  (722, 1),\n",
       "  (723, 1),\n",
       "  (724, 2),\n",
       "  (725, 2),\n",
       "  (726, 3),\n",
       "  (727, 2),\n",
       "  (728, 2),\n",
       "  (729, 2),\n",
       "  (730, 1),\n",
       "  (731, 1),\n",
       "  (732, 1),\n",
       "  (733, 1),\n",
       "  (734, 1),\n",
       "  (735, 1),\n",
       "  (736, 1),\n",
       "  (737, 2),\n",
       "  (738, 1),\n",
       "  (739, 1),\n",
       "  (740, 1),\n",
       "  (741, 1),\n",
       "  (742, 1),\n",
       "  (743, 1),\n",
       "  (744, 1),\n",
       "  (745, 5),\n",
       "  (746, 1),\n",
       "  (747, 1),\n",
       "  (748, 1),\n",
       "  (749, 2),\n",
       "  (750, 1),\n",
       "  (751, 1),\n",
       "  (752, 1),\n",
       "  (753, 1),\n",
       "  (754, 1),\n",
       "  (755, 1),\n",
       "  (756, 1),\n",
       "  (757, 1),\n",
       "  (758, 2),\n",
       "  (759, 1),\n",
       "  (760, 3),\n",
       "  (761, 8),\n",
       "  (762, 1),\n",
       "  (763, 2),\n",
       "  (764, 1),\n",
       "  (765, 2),\n",
       "  (766, 1),\n",
       "  (767, 1),\n",
       "  (768, 4),\n",
       "  (769, 1),\n",
       "  (770, 2),\n",
       "  (771, 2),\n",
       "  (772, 1),\n",
       "  (773, 1),\n",
       "  (774, 1),\n",
       "  (775, 1),\n",
       "  (776, 1),\n",
       "  (777, 1),\n",
       "  (778, 1),\n",
       "  (779, 4),\n",
       "  (780, 2),\n",
       "  (781, 2),\n",
       "  (782, 1),\n",
       "  (783, 3),\n",
       "  (784, 1),\n",
       "  (785, 1),\n",
       "  (786, 1),\n",
       "  (787, 1),\n",
       "  (788, 2),\n",
       "  (789, 1),\n",
       "  (790, 4),\n",
       "  (791, 1),\n",
       "  (792, 3),\n",
       "  (793, 1),\n",
       "  (794, 1),\n",
       "  (795, 1),\n",
       "  (796, 1),\n",
       "  (797, 1),\n",
       "  (798, 11),\n",
       "  (799, 2),\n",
       "  (800, 2),\n",
       "  (801, 1),\n",
       "  (802, 1),\n",
       "  (803, 1),\n",
       "  (804, 1),\n",
       "  (805, 1),\n",
       "  (806, 1),\n",
       "  (807, 1),\n",
       "  (808, 2),\n",
       "  (809, 2),\n",
       "  (810, 1),\n",
       "  (811, 2),\n",
       "  (812, 3),\n",
       "  (813, 1),\n",
       "  (814, 1),\n",
       "  (815, 1),\n",
       "  (816, 1),\n",
       "  (817, 1),\n",
       "  (818, 3),\n",
       "  (819, 5),\n",
       "  (820, 2),\n",
       "  (821, 1),\n",
       "  (822, 1),\n",
       "  (823, 1),\n",
       "  (824, 2),\n",
       "  (825, 1),\n",
       "  (826, 1),\n",
       "  (827, 1),\n",
       "  (828, 2),\n",
       "  (829, 1),\n",
       "  (830, 1),\n",
       "  (831, 1),\n",
       "  (832, 3),\n",
       "  (833, 1),\n",
       "  (834, 1),\n",
       "  (835, 4),\n",
       "  (836, 3),\n",
       "  (837, 1),\n",
       "  (838, 2),\n",
       "  (839, 1),\n",
       "  (840, 5),\n",
       "  (841, 4),\n",
       "  (842, 1),\n",
       "  (843, 1),\n",
       "  (844, 1),\n",
       "  (845, 2),\n",
       "  (846, 1),\n",
       "  (847, 1),\n",
       "  (848, 1),\n",
       "  (849, 1),\n",
       "  (850, 1),\n",
       "  (851, 1),\n",
       "  (852, 2),\n",
       "  (853, 1),\n",
       "  (854, 1),\n",
       "  (855, 1),\n",
       "  (856, 1),\n",
       "  (857, 1),\n",
       "  (858, 1),\n",
       "  (859, 1),\n",
       "  (860, 2),\n",
       "  (861, 2),\n",
       "  (862, 1),\n",
       "  (863, 1),\n",
       "  (864, 1),\n",
       "  (865, 2),\n",
       "  (866, 5),\n",
       "  (867, 1),\n",
       "  (868, 3),\n",
       "  (869, 1),\n",
       "  (870, 2),\n",
       "  (871, 1),\n",
       "  (872, 1),\n",
       "  (873, 3),\n",
       "  (874, 1),\n",
       "  (875, 1),\n",
       "  (876, 1),\n",
       "  (877, 1),\n",
       "  (878, 37),\n",
       "  (879, 1),\n",
       "  (880, 1),\n",
       "  (881, 1),\n",
       "  (882, 1),\n",
       "  (883, 2),\n",
       "  (884, 1),\n",
       "  (885, 1),\n",
       "  (886, 1),\n",
       "  (887, 1),\n",
       "  (888, 1),\n",
       "  (889, 1),\n",
       "  (890, 1),\n",
       "  (891, 1),\n",
       "  (892, 1),\n",
       "  (893, 1),\n",
       "  (894, 1),\n",
       "  (895, 2),\n",
       "  (896, 4),\n",
       "  (897, 1),\n",
       "  (898, 1),\n",
       "  (899, 1),\n",
       "  (900, 1),\n",
       "  (901, 1),\n",
       "  (902, 1),\n",
       "  (903, 1),\n",
       "  (904, 1),\n",
       "  (905, 2),\n",
       "  (906, 9),\n",
       "  (907, 7),\n",
       "  (908, 5),\n",
       "  (909, 1),\n",
       "  (910, 2),\n",
       "  (911, 1),\n",
       "  (912, 1),\n",
       "  (913, 2),\n",
       "  (914, 1),\n",
       "  (915, 4),\n",
       "  (916, 1),\n",
       "  (917, 1),\n",
       "  (918, 2),\n",
       "  (919, 1),\n",
       "  (920, 1),\n",
       "  (921, 2),\n",
       "  (922, 4),\n",
       "  (923, 1),\n",
       "  (924, 2),\n",
       "  (925, 2),\n",
       "  (926, 1),\n",
       "  (927, 2),\n",
       "  (928, 1),\n",
       "  (929, 1),\n",
       "  (930, 1),\n",
       "  (931, 1),\n",
       "  (932, 1),\n",
       "  (933, 20),\n",
       "  (934, 5),\n",
       "  (935, 1),\n",
       "  (936, 1),\n",
       "  (937, 1),\n",
       "  (938, 1),\n",
       "  (939, 1),\n",
       "  (940, 1),\n",
       "  (941, 1),\n",
       "  (942, 1),\n",
       "  (943, 1),\n",
       "  (944, 1),\n",
       "  (945, 1),\n",
       "  (946, 3),\n",
       "  (947, 2),\n",
       "  (948, 1),\n",
       "  (949, 2),\n",
       "  (950, 2),\n",
       "  (951, 1),\n",
       "  (952, 1),\n",
       "  (953, 1),\n",
       "  (954, 1),\n",
       "  (955, 1),\n",
       "  (956, 1),\n",
       "  (957, 1),\n",
       "  (958, 3),\n",
       "  (959, 1),\n",
       "  (960, 1),\n",
       "  (961, 3),\n",
       "  (962, 1),\n",
       "  (963, 1),\n",
       "  (964, 1),\n",
       "  (965, 2),\n",
       "  (966, 1),\n",
       "  (967, 1),\n",
       "  (968, 1),\n",
       "  (969, 3),\n",
       "  (970, 20),\n",
       "  (971, 6),\n",
       "  (972, 2),\n",
       "  (973, 1),\n",
       "  (974, 1),\n",
       "  (975, 2),\n",
       "  (976, 1),\n",
       "  (977, 1),\n",
       "  (978, 1),\n",
       "  (979, 2),\n",
       "  (980, 1),\n",
       "  (981, 2),\n",
       "  (982, 1),\n",
       "  (983, 2),\n",
       "  (984, 1),\n",
       "  (985, 1),\n",
       "  (986, 1),\n",
       "  (987, 1),\n",
       "  (988, 1),\n",
       "  (989, 1),\n",
       "  (990, 1),\n",
       "  (991, 1),\n",
       "  (992, 1),\n",
       "  (993, 4),\n",
       "  (994, 1),\n",
       "  (995, 1),\n",
       "  (996, 4),\n",
       "  (997, 3),\n",
       "  (998, 4),\n",
       "  (999, 1),\n",
       "  (1000, 1),\n",
       "  (1001, 1),\n",
       "  (1002, 1),\n",
       "  (1003, 1),\n",
       "  (1004, 2),\n",
       "  (1005, 1),\n",
       "  (1006, 1),\n",
       "  (1007, 1),\n",
       "  (1008, 1),\n",
       "  (1009, 4),\n",
       "  (1010, 1),\n",
       "  (1011, 1),\n",
       "  (1012, 1),\n",
       "  (1013, 1),\n",
       "  (1014, 1),\n",
       "  (1015, 1),\n",
       "  (1016, 1),\n",
       "  (1017, 1),\n",
       "  (1018, 1),\n",
       "  (1019, 1),\n",
       "  (1020, 1),\n",
       "  (1021, 2),\n",
       "  (1022, 6),\n",
       "  (1023, 1),\n",
       "  (1024, 2),\n",
       "  (1025, 2),\n",
       "  (1026, 1),\n",
       "  (1027, 1),\n",
       "  (1028, 5),\n",
       "  (1029, 1),\n",
       "  (1030, 1),\n",
       "  (1031, 1),\n",
       "  (1032, 1),\n",
       "  (1033, 4),\n",
       "  (1034, 1),\n",
       "  (1035, 1),\n",
       "  (1036, 1),\n",
       "  (1037, 1),\n",
       "  (1038, 1),\n",
       "  (1039, 1),\n",
       "  (1040, 1),\n",
       "  (1041, 1),\n",
       "  (1042, 1),\n",
       "  (1043, 5),\n",
       "  (1044, 1),\n",
       "  (1045, 1),\n",
       "  (1046, 2),\n",
       "  (1047, 2),\n",
       "  (1048, 1),\n",
       "  (1049, 1),\n",
       "  (1050, 1),\n",
       "  (1051, 3),\n",
       "  (1052, 3),\n",
       "  (1053, 1),\n",
       "  (1054, 1),\n",
       "  (1055, 1),\n",
       "  (1056, 6),\n",
       "  (1057, 2),\n",
       "  (1058, 13),\n",
       "  (1059, 1),\n",
       "  (1060, 4),\n",
       "  (1061, 4),\n",
       "  (1062, 2),\n",
       "  (1063, 1),\n",
       "  (1064, 4),\n",
       "  (1065, 1),\n",
       "  (1066, 1),\n",
       "  (1067, 1),\n",
       "  (1068, 3),\n",
       "  (1069, 3),\n",
       "  (1070, 5),\n",
       "  (1071, 1),\n",
       "  (1072, 1),\n",
       "  (1073, 1),\n",
       "  (1074, 1),\n",
       "  (1075, 1),\n",
       "  (1076, 1),\n",
       "  (1077, 1),\n",
       "  (1078, 1),\n",
       "  (1079, 1),\n",
       "  (1080, 1),\n",
       "  (1081, 1),\n",
       "  (1082, 2),\n",
       "  (1083, 2),\n",
       "  (1084, 12),\n",
       "  (1085, 6),\n",
       "  (1086, 1),\n",
       "  (1087, 5),\n",
       "  (1088, 1),\n",
       "  (1089, 1),\n",
       "  (1090, 6),\n",
       "  (1091, 3),\n",
       "  (1092, 1),\n",
       "  (1093, 1),\n",
       "  (1094, 2),\n",
       "  (1095, 3),\n",
       "  (1096, 3),\n",
       "  (1097, 7),\n",
       "  (1098, 2),\n",
       "  (1099, 1),\n",
       "  (1100, 1),\n",
       "  (1101, 1),\n",
       "  (1102, 2),\n",
       "  (1103, 3),\n",
       "  (1104, 2),\n",
       "  (1105, 1),\n",
       "  (1106, 1),\n",
       "  (1107, 5),\n",
       "  (1108, 2),\n",
       "  (1109, 1),\n",
       "  (1110, 1),\n",
       "  (1111, 6),\n",
       "  (1112, 2),\n",
       "  (1113, 1),\n",
       "  (1114, 1),\n",
       "  (1115, 1),\n",
       "  (1116, 2),\n",
       "  (1117, 2),\n",
       "  (1118, 1),\n",
       "  (1119, 2),\n",
       "  (1120, 1),\n",
       "  (1121, 1),\n",
       "  (1122, 5),\n",
       "  (1123, 1),\n",
       "  (1124, 1),\n",
       "  (1125, 1),\n",
       "  (1126, 3),\n",
       "  (1127, 3),\n",
       "  (1128, 4),\n",
       "  (1129, 1),\n",
       "  (1130, 3),\n",
       "  (1131, 2),\n",
       "  (1132, 3),\n",
       "  (1133, 1),\n",
       "  (1134, 1),\n",
       "  (1135, 1),\n",
       "  (1136, 1),\n",
       "  (1137, 1),\n",
       "  (1138, 1),\n",
       "  (1139, 1),\n",
       "  (1140, 1),\n",
       "  (1141, 1),\n",
       "  (1142, 1),\n",
       "  (1143, 7),\n",
       "  (1144, 2),\n",
       "  (1145, 1),\n",
       "  (1146, 1),\n",
       "  (1147, 1),\n",
       "  (1148, 4),\n",
       "  (1149, 1),\n",
       "  (1150, 1),\n",
       "  (1151, 1),\n",
       "  (1152, 1),\n",
       "  (1153, 2),\n",
       "  (1154, 4),\n",
       "  (1155, 2),\n",
       "  (1156, 1),\n",
       "  (1157, 1),\n",
       "  (1158, 2),\n",
       "  (1159, 1),\n",
       "  (1160, 1),\n",
       "  (1161, 1),\n",
       "  (1162, 5),\n",
       "  (1163, 6),\n",
       "  (1164, 1),\n",
       "  (1165, 1),\n",
       "  (1166, 3),\n",
       "  (1167, 2),\n",
       "  (1168, 1),\n",
       "  (1169, 1),\n",
       "  (1170, 1),\n",
       "  (1171, 3),\n",
       "  (1172, 1),\n",
       "  (1173, 1),\n",
       "  (1174, 1),\n",
       "  (1175, 1),\n",
       "  (1176, 3),\n",
       "  (1177, 2),\n",
       "  (1178, 4),\n",
       "  (1179, 1),\n",
       "  (1180, 2),\n",
       "  (1181, 4),\n",
       "  (1182, 1),\n",
       "  (1183, 5),\n",
       "  (1184, 3),\n",
       "  (1185, 2),\n",
       "  (1186, 1),\n",
       "  (1187, 1),\n",
       "  (1188, 2),\n",
       "  (1189, 2),\n",
       "  (1190, 4),\n",
       "  (1191, 2),\n",
       "  (1192, 1),\n",
       "  (1193, 3),\n",
       "  (1194, 1),\n",
       "  (1195, 3)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many tokens and documents we have to train on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 8644\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "We are ready to train the LDA model. We will first discuss how to set some of\n",
    "the training parameters.\n",
    "\n",
    "First of all, the elephant in the room: how many topics do I need? There is\n",
    "really no easy answer for this, it will depend on both your data and your\n",
    "application. I have used 10 topics here because I wanted to have a few topics\n",
    "that I could interpret and \"label\", and because that turned out to give me\n",
    "reasonably good results. You might not need to interpret all your topics, so\n",
    "you could use a large number of topics, for example 100.\n",
    "\n",
    "``chunksize`` controls how many documents are processed at a time in the\n",
    "training algorithm. Increasing chunksize will speed up training, at least as\n",
    "long as the chunk of documents easily fit into memory. I've set ``chunksize =\n",
    "2000``, which is more than the amount of documents, so I process all the\n",
    "data in one go. Chunksize can however influence the quality of the model, as\n",
    "discussed in Hoffman and co-authors [2], but the difference was not\n",
    "substantial in this case.\n",
    "\n",
    "``passes`` controls how often we train the model on the entire corpus.\n",
    "Another word for passes might be \"epochs\". ``iterations`` is somewhat\n",
    "technical, but essentially it controls how often we repeat a particular loop\n",
    "over each document. It is important to set the number of \"passes\" and\n",
    "\"iterations\" high enough.\n",
    "\n",
    "I suggest the following way to choose iterations and passes. First, enable\n",
    "logging (as described in many Gensim tutorials), and set ``eval_every = 1``\n",
    "in ``LdaModel``. When training the model look for a line in the log that\n",
    "looks something like this::\n",
    "\n",
    "   2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations\n",
    "\n",
    "If you set ``passes = 20`` you will see this line 20 times. Make sure that by\n",
    "the final passes, most of the documents have converged. So you want to choose\n",
    "both passes and iterations to be high enough for this to happen.\n",
    "\n",
    "We set ``alpha = 'auto'`` and ``eta = 'auto'``. Again this is somewhat\n",
    "technical, but essentially we are automatically learning two parameters in\n",
    "the model that we usually would have to specify explicitly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: u'2n',\n",
       " 1: u'_c',\n",
       " 2: u'a2',\n",
       " 3: u'a_follows',\n",
       " 4: u'ability',\n",
       " 5: u'abu',\n",
       " 6: u'abu_mostafa',\n",
       " 7: u'access',\n",
       " 8: u'accommodate',\n",
       " 9: u'according',\n",
       " 10: u'according_to',\n",
       " 11: u'accumulated',\n",
       " 12: u'acknowledgement',\n",
       " 13: u'acknowledgement_this',\n",
       " 14: u'addison',\n",
       " 15: u'addison_wesley',\n",
       " 16: u'afosr',\n",
       " 17: u'aip',\n",
       " 18: u'air',\n",
       " 19: u'air_force',\n",
       " 20: u'all_possible',\n",
       " 21: u'although',\n",
       " 22: u'american',\n",
       " 23: u'american_institute',\n",
       " 24: u'amount',\n",
       " 25: u'an_arbitrary',\n",
       " 26: u'analog',\n",
       " 27: u'anyway',\n",
       " 28: u'ao',\n",
       " 29: u'appears',\n",
       " 30: u'appendix',\n",
       " 31: u'approximately',\n",
       " 32: u'arbitrary',\n",
       " 33: u'architecture',\n",
       " 34: u'are_interested',\n",
       " 35: u'arise',\n",
       " 36: u'aspect',\n",
       " 37: u'assume',\n",
       " 38: u'assumed',\n",
       " 39: u'assumption',\n",
       " 40: u'asymptotic',\n",
       " 41: u'automaton',\n",
       " 42: u'available',\n",
       " 43: u'away',\n",
       " 44: u'axe',\n",
       " 45: u'basic',\n",
       " 46: u'become',\n",
       " 47: u'becomes',\n",
       " 48: u'benefit',\n",
       " 49: u'ber',\n",
       " 50: u'best',\n",
       " 51: u'big',\n",
       " 52: u'binary',\n",
       " 53: u'biological',\n",
       " 54: u'bit',\n",
       " 55: u'bit_per',\n",
       " 56: u'boolean',\n",
       " 57: u'boolean_function',\n",
       " 58: u'bound',\n",
       " 59: u'bt',\n",
       " 60: u'ca',\n",
       " 61: u'ca_abstract',\n",
       " 62: u'california',\n",
       " 63: u'california_institute',\n",
       " 64: u'cannot',\n",
       " 65: u'capable',\n",
       " 66: u'carried',\n",
       " 67: u'ceedings',\n",
       " 68: u'cell',\n",
       " 69: u'choosing',\n",
       " 70: u'circuit',\n",
       " 71: u'clearly',\n",
       " 72: u'collective',\n",
       " 73: u'compare',\n",
       " 74: u'complete',\n",
       " 75: u'complex',\n",
       " 76: u'complexity',\n",
       " 77: u'computing',\n",
       " 78: u'concept',\n",
       " 79: u'conclude',\n",
       " 80: u'conclude_that',\n",
       " 81: u'conference',\n",
       " 82: u'connected',\n",
       " 83: u'connectivity',\n",
       " 84: u'consequence',\n",
       " 85: u'contribution',\n",
       " 86: u'convenience',\n",
       " 87: u'coordinate',\n",
       " 88: u'corresponds',\n",
       " 89: u'corresponds_to',\n",
       " 90: u'define',\n",
       " 91: u'defines',\n",
       " 92: u'definition',\n",
       " 93: u'denker',\n",
       " 94: u'denote',\n",
       " 95: u'depend',\n",
       " 96: u'depend_on',\n",
       " 97: u'depending',\n",
       " 98: u'depending_on',\n",
       " 99: u'depends',\n",
       " 100: u'depends_on',\n",
       " 101: u'describes',\n",
       " 102: u'designed',\n",
       " 103: u'desired',\n",
       " 104: u'diagonal',\n",
       " 105: u'difference_between',\n",
       " 106: u'directly',\n",
       " 107: u'discussing',\n",
       " 108: u'disorder',\n",
       " 109: u'distinguishing',\n",
       " 110: u'doing',\n",
       " 111: u'drawn',\n",
       " 112: u'drawn_from',\n",
       " 113: u'e',\n",
       " 114: u'ea',\n",
       " 115: u'easy',\n",
       " 116: u'ed',\n",
       " 117: u'edge',\n",
       " 118: u'el',\n",
       " 119: u'element',\n",
       " 120: u'en',\n",
       " 121: u'end',\n",
       " 122: u'enough',\n",
       " 123: u'ensemble',\n",
       " 124: u'entire',\n",
       " 125: u'entropy',\n",
       " 126: u'environment',\n",
       " 127: u'equiv',\n",
       " 128: u'equivalent',\n",
       " 129: u'essentially',\n",
       " 130: u'estimate',\n",
       " 131: u'evaluate',\n",
       " 132: u'eventually',\n",
       " 133: u'everything',\n",
       " 134: u'expand',\n",
       " 135: u'expected',\n",
       " 136: u'expected_value',\n",
       " 137: u'exposition',\n",
       " 138: u'expression',\n",
       " 139: u'extraction',\n",
       " 140: u'extreme',\n",
       " 141: u'fact',\n",
       " 142: u'fact_that',\n",
       " 143: u'final',\n",
       " 144: u'finite',\n",
       " 145: u'finite_automaton',\n",
       " 146: u'fir',\n",
       " 147: u'follows',\n",
       " 148: u'force',\n",
       " 149: u'formal',\n",
       " 150: u'frequency',\n",
       " 151: u'furthermore',\n",
       " 152: u'gate',\n",
       " 153: u'generate',\n",
       " 154: u'generated',\n",
       " 155: u'generated_by',\n",
       " 156: u'generating',\n",
       " 157: u'get',\n",
       " 158: u'getting',\n",
       " 159: u'global',\n",
       " 160: u'go',\n",
       " 161: u'go_through',\n",
       " 162: u'going',\n",
       " 163: u'gradually',\n",
       " 164: u'grant',\n",
       " 165: u'graph',\n",
       " 166: u'h2',\n",
       " 167: u'hand',\n",
       " 168: u'handle',\n",
       " 169: u'he',\n",
       " 170: u'hence',\n",
       " 171: u'her',\n",
       " 172: u'hi',\n",
       " 173: u'hill',\n",
       " 174: u'hold',\n",
       " 175: u'huge',\n",
       " 176: u'idea',\n",
       " 177: u'ieee',\n",
       " 178: u'ieee_trans',\n",
       " 179: u'illustrate',\n",
       " 180: u'imple',\n",
       " 181: u'implemented',\n",
       " 182: u'imposes',\n",
       " 183: u'indeed',\n",
       " 184: u'independent',\n",
       " 185: u'independently',\n",
       " 186: u'infinity',\n",
       " 187: u'infor',\n",
       " 188: u'information_about',\n",
       " 189: u'institute',\n",
       " 190: u'interaction',\n",
       " 191: u'interested',\n",
       " 192: u'internal',\n",
       " 193: u'introduce',\n",
       " 194: u'involved',\n",
       " 195: u'it_own',\n",
       " 196: u'itself',\n",
       " 197: u'ity',\n",
       " 198: u'jl',\n",
       " 199: u'jr',\n",
       " 200: u'july',\n",
       " 201: u'just',\n",
       " 202: u'ki',\n",
       " 203: u'kind',\n",
       " 204: u'know',\n",
       " 205: u'knowledge',\n",
       " 206: u'label',\n",
       " 207: u'last',\n",
       " 208: u'le_than',\n",
       " 209: u'learn',\n",
       " 210: u'learned',\n",
       " 211: u'learning_rule',\n",
       " 212: u'learns',\n",
       " 213: u'length',\n",
       " 214: u'let',\n",
       " 215: u'let_denote',\n",
       " 216: u'li',\n",
       " 217: u'likely',\n",
       " 218: u'lim',\n",
       " 219: u'limit',\n",
       " 220: u'loaded',\n",
       " 221: u'log',\n",
       " 222: u'low',\n",
       " 223: u'lower',\n",
       " 224: u'lower_bound',\n",
       " 225: u'main',\n",
       " 226: u'main_result',\n",
       " 227: u'maximum',\n",
       " 228: u'mcgraw',\n",
       " 229: u'mcgraw_hill',\n",
       " 230: u'mead',\n",
       " 231: u'measure',\n",
       " 232: u'measured',\n",
       " 233: u'mechanism',\n",
       " 234: u'merely',\n",
       " 235: u'mostafa',\n",
       " 236: u'much_smaller',\n",
       " 237: u'must_be',\n",
       " 238: u'n2',\n",
       " 239: u'need',\n",
       " 240: u'needed',\n",
       " 241: u'neither',\n",
       " 242: u'neuron',\n",
       " 243: u'next',\n",
       " 244: u'next_section',\n",
       " 245: u'nl',\n",
       " 246: u'nor',\n",
       " 247: u'normalized',\n",
       " 248: u'notation',\n",
       " 249: u'num',\n",
       " 250: u'num_ber',\n",
       " 251: u'o0',\n",
       " 252: u'o1',\n",
       " 253: u'object',\n",
       " 254: u'obvious',\n",
       " 255: u'occur',\n",
       " 256: u'occurrence',\n",
       " 257: u'off',\n",
       " 258: u'off_diagonal',\n",
       " 259: u'office',\n",
       " 260: u'once',\n",
       " 261: u'oo',\n",
       " 262: u'opposite',\n",
       " 263: u'other_word',\n",
       " 264: u'otherwise',\n",
       " 265: u'overall',\n",
       " 266: u'own',\n",
       " 267: u'pasadena',\n",
       " 268: u'per',\n",
       " 269: u'perform',\n",
       " 270: u'perhaps',\n",
       " 271: u'physic',\n",
       " 272: u'picture',\n",
       " 273: u'pixel',\n",
       " 274: u'place',\n",
       " 275: u'plausible',\n",
       " 276: u'position',\n",
       " 277: u'powerful',\n",
       " 278: u'pp',\n",
       " 279: u'pr',\n",
       " 280: u'predicts',\n",
       " 281: u'principle',\n",
       " 282: u'pro',\n",
       " 283: u'pro_ceedings',\n",
       " 284: u'probability_distribution',\n",
       " 285: u'produce',\n",
       " 286: u'program',\n",
       " 287: u'projection',\n",
       " 288: u'proof',\n",
       " 289: u'prove',\n",
       " 290: u'provides',\n",
       " 291: u'pt',\n",
       " 292: u'putting',\n",
       " 293: u'quantitative',\n",
       " 294: u'r2',\n",
       " 295: u'random_variable',\n",
       " 296: u'randomness',\n",
       " 297: u'range',\n",
       " 298: u'rather',\n",
       " 299: u'rather_than',\n",
       " 300: u'ready',\n",
       " 301: u'recall',\n",
       " 302: u'regardless',\n",
       " 303: u'relate',\n",
       " 304: u'relation',\n",
       " 305: u'relative',\n",
       " 306: u'relative_frequency',\n",
       " 307: u'replacement',\n",
       " 308: u'report',\n",
       " 309: u'represented',\n",
       " 310: u'represented_by',\n",
       " 311: u'respect',\n",
       " 312: u'respectively',\n",
       " 313: u'restrict',\n",
       " 314: u'restricted',\n",
       " 315: u'restriction',\n",
       " 316: u'rh',\n",
       " 317: u'roughly',\n",
       " 318: u'roughly_speaking',\n",
       " 319: u'rule',\n",
       " 320: u'run',\n",
       " 321: u'sample',\n",
       " 322: u'say',\n",
       " 323: u'scene',\n",
       " 324: u'scientific',\n",
       " 325: u'scientific_research',\n",
       " 326: u'search',\n",
       " 327: u'seen',\n",
       " 328: u'selected',\n",
       " 329: u'shall',\n",
       " 330: u'simulate',\n",
       " 331: u'smaller',\n",
       " 332: u'something',\n",
       " 333: u'sophisticated',\n",
       " 334: u'speaking',\n",
       " 335: u'specifies',\n",
       " 336: u'specify',\n",
       " 337: u'start',\n",
       " 338: u'starting',\n",
       " 339: u'statistic',\n",
       " 340: u'statistical',\n",
       " 341: u'statistically',\n",
       " 342: u'still',\n",
       " 343: u'string',\n",
       " 344: u'strong',\n",
       " 345: u'subset',\n",
       " 346: u'substituting',\n",
       " 347: u'suggest',\n",
       " 348: u'sum',\n",
       " 349: u'supported',\n",
       " 350: u'supported_by',\n",
       " 351: u'suppose',\n",
       " 352: u'supposed',\n",
       " 353: u'switching',\n",
       " 354: u'sx',\n",
       " 355: u'symmetry',\n",
       " 356: u'synapse',\n",
       " 357: u'synapsis',\n",
       " 358: u'taken',\n",
       " 359: u'technical',\n",
       " 360: u'technology',\n",
       " 361: u'technology_pasadena',\n",
       " 362: u'tell',\n",
       " 363: u'them',\n",
       " 364: u'themselves',\n",
       " 365: u'theorem',\n",
       " 366: u'tive',\n",
       " 367: u'together',\n",
       " 368: u'together_with',\n",
       " 369: u'total',\n",
       " 370: u'trans',\n",
       " 371: u'tune',\n",
       " 372: u'twice',\n",
       " 373: u'ues',\n",
       " 374: u'under_grant',\n",
       " 375: u'undirected',\n",
       " 376: u'uniform',\n",
       " 377: u'upper',\n",
       " 378: u'upper_bound',\n",
       " 379: u'us',\n",
       " 380: u'v2',\n",
       " 381: u'va',\n",
       " 382: u'val',\n",
       " 383: u'val_ues',\n",
       " 384: u'variance',\n",
       " 385: u'variation',\n",
       " 386: u'version',\n",
       " 387: u'versus',\n",
       " 388: u'vertex',\n",
       " 389: u'very_low',\n",
       " 390: u'view',\n",
       " 391: u'visual',\n",
       " 392: u'visual_scene',\n",
       " 393: u'vl',\n",
       " 394: u'vol',\n",
       " 395: u'we_define',\n",
       " 396: u'we_shall',\n",
       " 397: u'wesley',\n",
       " 398: u'what',\n",
       " 399: u'with_respect',\n",
       " 400: u'word',\n",
       " 401: u'work_wa',\n",
       " 402: u'wr',\n",
       " 403: u'write',\n",
       " 404: u'written',\n",
       " 405: u'written_a',\n",
       " 406: u'x2',\n",
       " 407: u'xi',\n",
       " 408: u'xl',\n",
       " 409: u'xn',\n",
       " 410: u'ya',\n",
       " 411: u'you',\n",
       " 412: u'yx',\n",
       " 413: u'zl',\n",
       " 414: u'2e',\n",
       " 415: u'2k',\n",
       " 416: u'a_well',\n",
       " 417: u'absence',\n",
       " 418: u'absolute',\n",
       " 419: u'acad',\n",
       " 420: u'acad_sci',\n",
       " 421: u'achieve',\n",
       " 422: u'ackley',\n",
       " 423: u'ackley_hinton',\n",
       " 424: u'acknowledgment',\n",
       " 425: u'across',\n",
       " 426: u'activation',\n",
       " 427: u'activation_function',\n",
       " 428: u'adaptive',\n",
       " 429: u'add',\n",
       " 430: u'added',\n",
       " 431: u'adding',\n",
       " 432: u'addition',\n",
       " 433: u'address',\n",
       " 434: u'adjacent',\n",
       " 435: u'adjust',\n",
       " 436: u'adjusting',\n",
       " 437: u'adjustment',\n",
       " 438: u'adopted',\n",
       " 439: u'advance',\n",
       " 440: u'advanced',\n",
       " 441: u'advanced_research',\n",
       " 442: u'allen',\n",
       " 443: u'allows',\n",
       " 444: u'almost',\n",
       " 445: u'alspector',\n",
       " 446: u'always',\n",
       " 447: u'am',\n",
       " 448: u'amherst',\n",
       " 449: u'amplifier',\n",
       " 450: u'an_important',\n",
       " 451: u'analog_circuit',\n",
       " 452: u'analog_cmos',\n",
       " 453: u'anderson',\n",
       " 454: u'annealing',\n",
       " 455: u'annealing_schedule',\n",
       " 456: u'another',\n",
       " 457: u'answer',\n",
       " 458: u'approximation',\n",
       " 459: u'area',\n",
       " 460: u'arranged',\n",
       " 461: u'arrangement',\n",
       " 462: u'array',\n",
       " 463: u'artificial',\n",
       " 464: u'artificial_neural',\n",
       " 465: u'aside',\n",
       " 466: u'assignment',\n",
       " 467: u'associative',\n",
       " 468: u'associative_memory',\n",
       " 469: u'assure',\n",
       " 470: u'asynchronous',\n",
       " 471: u'asynchronously',\n",
       " 472: u'ation',\n",
       " 473: u'attached',\n",
       " 474: u'attack',\n",
       " 475: u'attempt',\n",
       " 476: u'average_over',\n",
       " 477: u'averaged',\n",
       " 478: u'averaged_over',\n",
       " 479: u'avoid',\n",
       " 480: u'avoiding',\n",
       " 481: u'ay',\n",
       " 482: u'back',\n",
       " 483: u'bandwidth',\n",
       " 484: u'barto',\n",
       " 485: u'barto_sutton',\n",
       " 486: u'basis',\n",
       " 487: u'before',\n",
       " 488: u'behavior',\n",
       " 489: u'believe',\n",
       " 490: u'bell',\n",
       " 491: u'below',\n",
       " 492: u'berkeley',\n",
       " 493: u'berkeley_ca',\n",
       " 494: u'better',\n",
       " 495: u'bi',\n",
       " 496: u'biologically',\n",
       " 497: u'bipolar',\n",
       " 498: u'block',\n",
       " 499: u'block_diagram',\n",
       " 500: u'boltzmann',\n",
       " 501: u'boltzmann_machine',\n",
       " 502: u'book',\n",
       " 503: u'bottom',\n",
       " 504: u'brain',\n",
       " 505: u'branch',\n",
       " 506: u'called',\n",
       " 507: u'cambridge',\n",
       " 508: u'cambridge_ma',\n",
       " 509: u'capacitance',\n",
       " 510: u'capacitor',\n",
       " 511: u'ccd',\n",
       " 512: u'ch',\n",
       " 513: u'chance',\n",
       " 514: u'channel',\n",
       " 515: u'characteristic',\n",
       " 516: u'characterize',\n",
       " 517: u'characterized',\n",
       " 518: u'charge',\n",
       " 519: u'chip',\n",
       " 520: u'choice',\n",
       " 521: u'chose',\n",
       " 522: u'chosen',\n",
       " 523: u'cij',\n",
       " 524: u'clamping',\n",
       " 525: u'classification',\n",
       " 526: u'close',\n",
       " 527: u'closest',\n",
       " 528: u'cluster',\n",
       " 529: u'cmos',\n",
       " 530: u'cmos_process',\n",
       " 531: u'cmos_technology',\n",
       " 532: u'co',\n",
       " 533: u'coding',\n",
       " 534: u'cognition',\n",
       " 535: u'cognition_vol',\n",
       " 536: u'cognitive',\n",
       " 537: u'cognitive_science',\n",
       " 538: u'cohen',\n",
       " 539: u'coin',\n",
       " 540: u'collected',\n",
       " 541: u'column',\n",
       " 542: u'com',\n",
       " 543: u'comer',\n",
       " 544: u'communication',\n",
       " 545: u'compact',\n",
       " 546: u'compared',\n",
       " 547: u'comparison',\n",
       " 548: u'compatible',\n",
       " 549: u'compatible_with',\n",
       " 550: u'compete',\n",
       " 551: u'competition',\n",
       " 552: u'competitive',\n",
       " 553: u'competitive_learning',\n",
       " 554: u'complementary',\n",
       " 555: u'completed',\n",
       " 556: u'component',\n",
       " 557: u'compressing',\n",
       " 558: u'computational',\n",
       " 559: u'computational_ability',\n",
       " 560: u'computer_simulation',\n",
       " 561: u'condition',\n",
       " 562: u'conductance',\n",
       " 563: u'connect',\n",
       " 564: u'connection',\n",
       " 565: u'connection_strength',\n",
       " 566: u'connects',\n",
       " 567: u'considered',\n",
       " 568: u'consisted',\n",
       " 569: u'consists',\n",
       " 570: u'continuous',\n",
       " 571: u'contrast',\n",
       " 572: u'contributing',\n",
       " 573: u'control',\n",
       " 574: u'controlled',\n",
       " 575: u'controlled_by',\n",
       " 576: u'controlling',\n",
       " 577: u'convention',\n",
       " 578: u'converging',\n",
       " 579: u'convert',\n",
       " 580: u'corn',\n",
       " 581: u'correct',\n",
       " 582: u'correct_answer',\n",
       " 583: u'correction',\n",
       " 584: u'correlated',\n",
       " 585: u'correlation',\n",
       " 586: u'correspond',\n",
       " 587: u'correspondence',\n",
       " 588: u'could_be',\n",
       " 589: u'count',\n",
       " 590: u'counter',\n",
       " 591: u'counting',\n",
       " 592: u'coupled',\n",
       " 593: u'creates',\n",
       " 594: u'credit',\n",
       " 595: u'criterion',\n",
       " 596: u'critic',\n",
       " 597: u'cumulative',\n",
       " 598: u'current_source',\n",
       " 599: u'currently',\n",
       " 600: u'cut',\n",
       " 601: u'cut_off',\n",
       " 602: u'cyber',\n",
       " 603: u'cycle',\n",
       " 604: u'dc',\n",
       " 605: u'de',\n",
       " 606: u'decay',\n",
       " 607: u'decide',\n",
       " 608: u'decrease',\n",
       " 609: u'decreased',\n",
       " 610: u'decrement',\n",
       " 611: u'demonstrate',\n",
       " 612: u'department',\n",
       " 613: u'dependence',\n",
       " 614: u'dependent',\n",
       " 615: u'describe',\n",
       " 616: u'described_here',\n",
       " 617: u'design',\n",
       " 618: u'detector',\n",
       " 619: u'determines',\n",
       " 620: u'deterministic',\n",
       " 621: u'developed',\n",
       " 622: u'development',\n",
       " 623: u'device',\n",
       " 624: u'diagram',\n",
       " 625: u'differential',\n",
       " 626: u'differs',\n",
       " 627: u'difficult',\n",
       " 628: u'diffusion',\n",
       " 629: u'digital',\n",
       " 630: u'directed',\n",
       " 631: u'discovering',\n",
       " 632: u'discovery',\n",
       " 633: u'dissertation',\n",
       " 634: u'distributed',\n",
       " 635: u'diverse',\n",
       " 636: u'divide',\n",
       " 637: u'divided',\n",
       " 638: u'division',\n",
       " 639: u'do_not',\n",
       " 640: u'doctoral',\n",
       " 641: u'domain',\n",
       " 642: u'dominance',\n",
       " 643: u'double',\n",
       " 644: u'down',\n",
       " 645: u'dynamic',\n",
       " 646: u'dynamic_range',\n",
       " 647: u'ease',\n",
       " 648: u'easily',\n",
       " 649: u'edited',\n",
       " 650: u'edited_by',\n",
       " 651: u'ee',\n",
       " 652: u'effective',\n",
       " 653: u'effectively',\n",
       " 654: u'efficient',\n",
       " 655: u'effort',\n",
       " 656: u'either',\n",
       " 657: u'electric',\n",
       " 658: u'electron',\n",
       " 659: u'electronic',\n",
       " 660: u'electronics',\n",
       " 661: u'emergent',\n",
       " 662: u'emergent_collective',\n",
       " 663: u'employed',\n",
       " 664: u'engineer',\n",
       " 665: u'ent',\n",
       " 666: u'eq',\n",
       " 667: u'equal',\n",
       " 668: u'er',\n",
       " 669: u'especially',\n",
       " 670: u'essential',\n",
       " 671: u'evaluation',\n",
       " 672: u'exceeded',\n",
       " 673: u'except',\n",
       " 674: u'excess',\n",
       " 675: u'expense',\n",
       " 676: u'explicit',\n",
       " 677: u'exploration',\n",
       " 678: u'explored',\n",
       " 679: u'exponentially',\n",
       " 680: u'extended',\n",
       " 681: u'externally',\n",
       " 682: u'ey',\n",
       " 683: u'f_',\n",
       " 684: u'factor',\n",
       " 685: u'family',\n",
       " 686: u'favorable',\n",
       " 687: u'feature_detector',\n",
       " 688: u'fed',\n",
       " 689: u'fed_into',\n",
       " 690: u'feedback',\n",
       " 691: u'ff',\n",
       " 692: u'fi',\n",
       " 693: u'fig',\n",
       " 694: u'fig_show',\n",
       " 695: u'finally',\n",
       " 696: u'finding',\n",
       " 697: u'flip',\n",
       " 698: u'floating',\n",
       " 699: u'focus',\n",
       " 700: u'focus_on',\n",
       " 701: u'follow',\n",
       " 702: u'formation',\n",
       " 703: u'foundation',\n",
       " 704: u'fourth',\n",
       " 705: u'fraction',\n",
       " 706: u'free',\n",
       " 707: u'fully',\n",
       " 708: u'functionality',\n",
       " 709: u'fundamental',\n",
       " 710: u'future',\n",
       " 711: u'gain',\n",
       " 712: u'gaussian',\n",
       " 713: u'gaussian_distribution',\n",
       " 714: u'gaussian_noise',\n",
       " 715: u'gave',\n",
       " 716: u'generalize',\n",
       " 717: u'generalize_well',\n",
       " 718: u'geoffrey',\n",
       " 719: u'geoffrey_hinton',\n",
       " 720: u'get_stuck',\n",
       " 721: u'global_optimization',\n",
       " 722: u'globally',\n",
       " 723: u'goal',\n",
       " 724: u'good',\n",
       " 725: u'graded',\n",
       " 726: u'gradient',\n",
       " 727: u'greater',\n",
       " 728: u'greater_than',\n",
       " 729: u'grossberg',\n",
       " 730: u'guaranteed',\n",
       " 731: u'guidance',\n",
       " 732: u'guided',\n",
       " 733: u'had',\n",
       " 734: u'hall',\n",
       " 735: u'hart',\n",
       " 736: u'havior',\n",
       " 737: u'hebb',\n",
       " 738: u'hebbian',\n",
       " 739: u'help',\n",
       " 740: u'helped',\n",
       " 741: u'hidden',\n",
       " 742: u'hidden_unit',\n",
       " 743: u'highly',\n",
       " 744: u'hint',\n",
       " 745: u'hinton',\n",
       " 746: u'his',\n",
       " 747: u'histogram',\n",
       " 748: u'hoff',\n",
       " 749: u'hope',\n",
       " 750: u'hopefully',\n",
       " 751: u'hopfield',\n",
       " 752: u'horizontally',\n",
       " 753: u'i2',\n",
       " 754: u'identify',\n",
       " 755: u'ii',\n",
       " 756: u'ij',\n",
       " 757: u'ill',\n",
       " 758: u'illustrated',\n",
       " 759: u'implement',\n",
       " 760: u'implementable',\n",
       " 761: u'implementation',\n",
       " 762: u'implementing',\n",
       " 763: u'improve',\n",
       " 764: u'improvement',\n",
       " 765: u'include',\n",
       " 766: u'includes',\n",
       " 767: u'incomplete',\n",
       " 768: u'increase',\n",
       " 769: u'increasingly',\n",
       " 770: u'increment',\n",
       " 771: u'incremented',\n",
       " 772: u'individual',\n",
       " 773: u'influenced',\n",
       " 774: u'influenced_by',\n",
       " 775: u'inherent',\n",
       " 776: u'inhibition',\n",
       " 777: u'initially',\n",
       " 778: u'inset',\n",
       " 779: u'inst',\n",
       " 780: u'instead',\n",
       " 781: u'integrated',\n",
       " 782: u'integrated_circuit',\n",
       " 783: u'integrator',\n",
       " 784: u'interconnected',\n",
       " 785: u'interesting',\n",
       " 786: u'internal_representation',\n",
       " 787: u'interpolate',\n",
       " 788: u'inverse',\n",
       " 789: u'issue',\n",
       " 790: u'j_',\n",
       " 791: u'ji',\n",
       " 792: u'jitter',\n",
       " 793: u'keeping',\n",
       " 794: u'larger',\n",
       " 795: u'lastly',\n",
       " 796: u'later',\n",
       " 797: u'latter',\n",
       " 798: u'layer',\n",
       " 799: u'layout',\n",
       " 800: u'leading',\n",
       " 801: u'leaky',\n",
       " 802: u'leaming',\n",
       " 803: u'led',\n",
       " 804: u'left',\n",
       " 805: u'lett',\n",
       " 806: u'll',\n",
       " 807: u'lo',\n",
       " 808: u'local_minimum',\n",
       " 809: u'locally',\n",
       " 810: u'logarithmic',\n",
       " 811: u'logic',\n",
       " 812: u'long',\n",
       " 813: u'long_term',\n",
       " 814: u'looking',\n",
       " 815: u'looking_at',\n",
       " 816: u'low_pas',\n",
       " 817: u'low_power',\n",
       " 818: u'ma',\n",
       " 819: u'machine',\n",
       " 820: u'magnitude',\n",
       " 821: u'mahowald',\n",
       " 822: u'major',\n",
       " 823: u'making',\n",
       " 824: u'man',\n",
       " 825: u'manipulated',\n",
       " 826: u'many_researcher',\n",
       " 827: u'mass',\n",
       " 828: u'matched',\n",
       " 829: u'matrix',\n",
       " 830: u'maximization',\n",
       " 831: u'mcclelland',\n",
       " 832: u'memory',\n",
       " 833: u'mentioned',\n",
       " 834: u'merit',\n",
       " 835: u'micron',\n",
       " 836: u'microscopic',\n",
       " 837: u'microstructure',\n",
       " 838: u'might',\n",
       " 839: u'might_be',\n",
       " 840: u'minimum',\n",
       " 841: u'minus',\n",
       " 842: u'mit',\n",
       " 843: u'mit_press',\n",
       " 844: u'mm',\n",
       " 845: u'mo',\n",
       " 846: u'modeling',\n",
       " 847: u'modification',\n",
       " 848: u'modifying',\n",
       " 849: u'molecular',\n",
       " 850: u'more_complex',\n",
       " 851: u'more_efficient',\n",
       " 852: u'more_than',\n",
       " 853: u'moreover',\n",
       " 854: u'moved',\n",
       " 855: u'moving',\n",
       " 856: u'moving_average',\n",
       " 857: u'multiple',\n",
       " 858: u'multiplied',\n",
       " 859: u'multiplied_by',\n",
       " 860: u'multiplying',\n",
       " 861: u'n',\n",
       " 862: u'natl',\n",
       " 863: u'natural',\n",
       " 864: u'naturally',\n",
       " 865: u'nearly',\n",
       " 866: u'necessary',\n",
       " 867: u'negative',\n",
       " 868: u'net',\n",
       " 869: u'neurodynamics',\n",
       " 870: u'neuromorphic',\n",
       " 871: u'neuronlike',\n",
       " 872: u'nevertheless',\n",
       " 873: u'new_york',\n",
       " 874: u'ning',\n",
       " 875: u'nj',\n",
       " 876: u'nj_abstract',\n",
       " 877: u'node',\n",
       " 878: u'noise',\n",
       " 879: u'normalization',\n",
       " 880: u'ny',\n",
       " 881: u'obtain',\n",
       " 882: u'occupied',\n",
       " 883: u'often',\n",
       " 884: u'operate',\n",
       " 885: u'operates',\n",
       " 886: u'optimal',\n",
       " 887: u'optimization',\n",
       " 888: u'organization',\n",
       " 889: u'oscillates',\n",
       " 890: u'oscillation',\n",
       " 891: u'others',\n",
       " 892: u'our_experiment',\n",
       " 893: u'pa',\n",
       " 894: u'pair',\n",
       " 895: u'paradigm',\n",
       " 896: u'parallel',\n",
       " 897: u'parallel_distributed',\n",
       " 898: u'particularly',\n",
       " 899: u'pas',\n",
       " 900: u'pathway',\n",
       " 901: u'percent',\n",
       " 902: u'percepttons',\n",
       " 903: u'performed',\n",
       " 904: u'performs',\n",
       " 905: u'permanent',\n",
       " 906: u'phase',\n",
       " 907: u'phenomenon',\n",
       " 908: u'physical',\n",
       " 909: u'physical_system',\n",
       " 910: u'physically',\n",
       " 911: u'picked',\n",
       " 912: u'plasticity',\n",
       " 913: u'plausibility',\n",
       " 914: u'play',\n",
       " 915: u'plus',\n",
       " 916: u'potential',\n",
       " 917: u'power',\n",
       " 918: u'preliminary',\n",
       " 919: u'preliminary_experiment',\n",
       " 920: u'preliminary_result',\n",
       " 921: u'presence',\n",
       " 922: u'presentation',\n",
       " 923: u'preserve',\n",
       " 924: u'press_cambridge',\n",
       " 925: u'previous',\n",
       " 926: u'previous_work',\n",
       " 927: u'previously',\n",
       " 928: u'previously_proposed',\n",
       " 929: u'prior',\n",
       " 930: u'proc',\n",
       " 931: u'proc_natl',\n",
       " 932: u'proce',\n",
       " 933: u'procedure',\n",
       " 934: u'proceeding',\n",
       " 935: u'proceeds',\n",
       " 936: u'produced',\n",
       " 937: u'progress',\n",
       " 938: u'propagate',\n",
       " 939: u'propagation',\n",
       " 940: u'proper',\n",
       " 941: u'properly',\n",
       " 942: u'proportion',\n",
       " 943: u'proportional',\n",
       " 944: u'propose',\n",
       " 945: u'proposed',\n",
       " 946: u'provide',\n",
       " 947: u'put',\n",
       " 948: u'quality',\n",
       " 949: u'quantity',\n",
       " 950: u'question',\n",
       " 951: u'quite',\n",
       " 952: u'radio',\n",
       " 953: u'randomly',\n",
       " 954: u'ratio',\n",
       " 955: u'real_time',\n",
       " 956: u'reasonable',\n",
       " 957: u'received',\n",
       " 958: u'recent',\n",
       " 959: u'recent_year',\n",
       " 960: u'record',\n",
       " 961: u'recurrent',\n",
       " 962: u'recurrent_network',\n",
       " 963: u'reduce',\n",
       " 964: u'reduced',\n",
       " 965: u'reducing',\n",
       " 966: u'refers',\n",
       " 967: u'refined',\n",
       " 968: u'regard',\n",
       " 969: u'region',\n",
       " 970: u'reinforcement',\n",
       " 971: u'reinforcement_learning',\n",
       " 972: u'related',\n",
       " 973: u'release',\n",
       " 974: u'replicate',\n",
       " 975: u'replication',\n",
       " 976: u'reported',\n",
       " 977: u'representing',\n",
       " 978: u'represents',\n",
       " 979: u'requires',\n",
       " 980: u'researcher',\n",
       " 981: u'resistance',\n",
       " 982: u'resistive',\n",
       " 983: u'resistor',\n",
       " 984: u'rest',\n",
       " 985: u'reward',\n",
       " 986: u'right',\n",
       " 987: u'rj',\n",
       " 988: u'rms',\n",
       " 989: u'robert',\n",
       " 990: u'robust',\n",
       " 991: u'role',\n",
       " 992: u'rosenblatt',\n",
       " 993: u'rumelhart',\n",
       " 994: u'sampled',\n",
       " 995: u'satisfying',\n",
       " 996: u'saturation',\n",
       " 997: u'scalar',\n",
       " 998: u'schedule',\n",
       " 999: u'schematically',\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-25 22:06:51,915 : INFO : using autotuned alpha, starting with [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "2019-10-25 22:06:51,925 : INFO : using serial LDA version on this node\n",
      "2019-10-25 22:06:51,942 : INFO : running online (multi-pass) LDA training, 10 topics, 20 passes over the supplied corpus of 1740 documents, updating model once every 1740 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "2019-10-25 22:06:51,944 : INFO : PROGRESS: pass 0, at document #1740/1740\n",
      "2019-10-25 22:07:11,857 : INFO : optimized alpha [0.06758672, 0.08475909, 0.10017212, 0.04867215, 0.0823116, 0.07340417, 0.06659031, 0.07348999, 0.069364965, 0.10157189]\n",
      "2019-10-25 22:07:11,873 : INFO : topic #3 (0.049): 0.006*\"cell\" + 0.005*\"image\" + 0.005*\"tree\" + 0.005*\"class\" + 0.004*\"map\" + 0.003*\"region\" + 0.003*\"constraint\" + 0.003*\"node\" + 0.003*\"neuron\" + 0.003*\"sample\"\n",
      "2019-10-25 22:07:11,875 : INFO : topic #6 (0.067): 0.010*\"image\" + 0.004*\"neuron\" + 0.004*\"map\" + 0.004*\"signal\" + 0.003*\"layer\" + 0.003*\"object\" + 0.003*\"classifier\" + 0.002*\"chip\" + 0.002*\"net\" + 0.002*\"estimate\"\n",
      "2019-10-25 22:07:11,877 : INFO : topic #1 (0.085): 0.008*\"neuron\" + 0.005*\"cell\" + 0.004*\"memory\" + 0.003*\"signal\" + 0.003*\"layer\" + 0.003*\"recognition\" + 0.003*\"noise\" + 0.003*\"dynamic\" + 0.003*\"control\" + 0.003*\"image\"\n",
      "2019-10-25 22:07:11,878 : INFO : topic #2 (0.100): 0.004*\"rule\" + 0.004*\"hidden\" + 0.003*\"cell\" + 0.003*\"net\" + 0.003*\"image\" + 0.003*\"layer\" + 0.003*\"component\" + 0.003*\"signal\" + 0.002*\"matrix\" + 0.002*\"gradient\"\n",
      "2019-10-25 22:07:11,880 : INFO : topic #9 (0.102): 0.004*\"class\" + 0.003*\"neuron\" + 0.003*\"matrix\" + 0.003*\"node\" + 0.003*\"noise\" + 0.003*\"signal\" + 0.003*\"cell\" + 0.003*\"classification\" + 0.003*\"sequence\" + 0.002*\"sample\"\n",
      "2019-10-25 22:07:11,882 : INFO : topic diff=1.176991, rho=1.000000\n",
      "2019-10-25 22:07:11,897 : INFO : PROGRESS: pass 1, at document #1740/1740\n",
      "2019-10-25 22:07:23,878 : INFO : optimized alpha [0.055742938, 0.06586793, 0.07113201, 0.041749086, 0.06615291, 0.061023038, 0.055550005, 0.06151561, 0.065470815, 0.07457213]\n",
      "2019-10-25 22:07:23,892 : INFO : topic #3 (0.042): 0.008*\"cell\" + 0.008*\"tree\" + 0.005*\"image\" + 0.005*\"class\" + 0.005*\"orientation\" + 0.004*\"map\" + 0.004*\"region\" + 0.003*\"node\" + 0.003*\"constraint\" + 0.003*\"classifier\"\n",
      "2019-10-25 22:07:23,893 : INFO : topic #6 (0.056): 0.015*\"image\" + 0.005*\"object\" + 0.004*\"map\" + 0.004*\"signal\" + 0.004*\"chip\" + 0.003*\"neuron\" + 0.003*\"analog\" + 0.003*\"visual\" + 0.003*\"recognition\" + 0.003*\"position\"\n",
      "2019-10-25 22:07:23,895 : INFO : topic #4 (0.066): 0.011*\"neuron\" + 0.006*\"response\" + 0.005*\"stimulus\" + 0.005*\"signal\" + 0.004*\"cell\" + 0.004*\"noise\" + 0.004*\"spike\" + 0.004*\"activity\" + 0.004*\"frequency\" + 0.003*\"hidden\"\n",
      "2019-10-25 22:07:23,896 : INFO : topic #2 (0.071): 0.006*\"rule\" + 0.004*\"hidden\" + 0.004*\"net\" + 0.003*\"component\" + 0.003*\"signal\" + 0.003*\"matrix\" + 0.003*\"layer\" + 0.003*\"cell\" + 0.003*\"gaussian\" + 0.003*\"gradient\"\n",
      "2019-10-25 22:07:23,898 : INFO : topic #9 (0.075): 0.004*\"class\" + 0.003*\"matrix\" + 0.003*\"classifier\" + 0.003*\"node\" + 0.003*\"classification\" + 0.003*\"sequence\" + 0.003*\"sample\" + 0.003*\"memory\" + 0.003*\"noise\" + 0.002*\"approximation\"\n",
      "2019-10-25 22:07:23,900 : INFO : topic diff=0.280158, rho=0.577350\n",
      "2019-10-25 22:07:23,914 : INFO : PROGRESS: pass 2, at document #1740/1740\n",
      "2019-10-25 22:07:33,130 : INFO : optimized alpha [0.048929855, 0.056282923, 0.05813169, 0.037692294, 0.058204643, 0.055263538, 0.050361753, 0.05472546, 0.06449549, 0.06260089]\n",
      "2019-10-25 22:07:33,143 : INFO : topic #3 (0.038): 0.010*\"tree\" + 0.009*\"cell\" + 0.007*\"orientation\" + 0.005*\"image\" + 0.005*\"map\" + 0.005*\"class\" + 0.004*\"region\" + 0.004*\"node\" + 0.004*\"field\" + 0.003*\"constraint\"\n",
      "2019-10-25 22:07:33,145 : INFO : topic #0 (0.049): 0.009*\"cell\" + 0.007*\"layer\" + 0.006*\"threshold\" + 0.006*\"neuron\" + 0.005*\"node\" + 0.005*\"net\" + 0.004*\"hidden\" + 0.004*\"connection\" + 0.004*\"circuit\" + 0.003*\"activation\"\n",
      "2019-10-25 22:07:33,147 : INFO : topic #4 (0.058): 0.014*\"neuron\" + 0.007*\"response\" + 0.007*\"stimulus\" + 0.006*\"spike\" + 0.006*\"cell\" + 0.005*\"signal\" + 0.005*\"noise\" + 0.005*\"activity\" + 0.004*\"frequency\" + 0.004*\"synaptic\"\n",
      "2019-10-25 22:07:33,148 : INFO : topic #9 (0.063): 0.005*\"class\" + 0.005*\"classifier\" + 0.004*\"classification\" + 0.004*\"node\" + 0.004*\"matrix\" + 0.003*\"sequence\" + 0.003*\"memory\" + 0.003*\"sample\" + 0.003*\"distance\" + 0.002*\"recognition\"\n",
      "2019-10-25 22:07:33,150 : INFO : topic #8 (0.064): 0.005*\"optimal\" + 0.004*\"control\" + 0.004*\"class\" + 0.004*\"bound\" + 0.003*\"action\" + 0.003*\"approximation\" + 0.003*\"estimate\" + 0.003*\"prediction\" + 0.003*\"policy\" + 0.003*\"gaussian\"\n",
      "2019-10-25 22:07:33,152 : INFO : topic diff=0.242479, rho=0.500000\n",
      "2019-10-25 22:07:33,166 : INFO : PROGRESS: pass 3, at document #1740/1740\n",
      "2019-10-25 22:07:41,183 : INFO : optimized alpha [0.044755425, 0.05087974, 0.0510261, 0.035079855, 0.053778533, 0.05234102, 0.047332663, 0.05056348, 0.063695155, 0.056231707]\n",
      "2019-10-25 22:07:41,196 : INFO : topic #3 (0.035): 0.012*\"tree\" + 0.010*\"cell\" + 0.008*\"orientation\" + 0.006*\"map\" + 0.005*\"image\" + 0.005*\"region\" + 0.004*\"class\" + 0.004*\"field\" + 0.004*\"node\" + 0.004*\"center\"\n",
      "2019-10-25 22:07:41,198 : INFO : topic #0 (0.045): 0.009*\"cell\" + 0.008*\"layer\" + 0.007*\"threshold\" + 0.007*\"neuron\" + 0.006*\"node\" + 0.005*\"net\" + 0.004*\"hidden\" + 0.004*\"connection\" + 0.004*\"circuit\" + 0.004*\"activation\"\n",
      "2019-10-25 22:07:41,199 : INFO : topic #4 (0.054): 0.015*\"neuron\" + 0.008*\"response\" + 0.008*\"stimulus\" + 0.007*\"cell\" + 0.007*\"spike\" + 0.006*\"signal\" + 0.005*\"activity\" + 0.005*\"noise\" + 0.005*\"frequency\" + 0.004*\"synaptic\"\n",
      "2019-10-25 22:07:41,201 : INFO : topic #9 (0.056): 0.006*\"class\" + 0.006*\"classifier\" + 0.004*\"classification\" + 0.004*\"node\" + 0.004*\"matrix\" + 0.003*\"sequence\" + 0.003*\"sample\" + 0.003*\"memory\" + 0.003*\"distance\" + 0.002*\"rbf\"\n",
      "2019-10-25 22:07:41,203 : INFO : topic #8 (0.064): 0.005*\"optimal\" + 0.004*\"control\" + 0.004*\"bound\" + 0.004*\"action\" + 0.004*\"approximation\" + 0.004*\"class\" + 0.004*\"estimate\" + 0.003*\"policy\" + 0.003*\"prediction\" + 0.003*\"sample\"\n",
      "2019-10-25 22:07:41,204 : INFO : topic diff=0.221174, rho=0.447214\n",
      "2019-10-25 22:07:41,220 : INFO : PROGRESS: pass 4, at document #1740/1740\n",
      "2019-10-25 22:07:48,970 : INFO : optimized alpha [0.04191794, 0.047500186, 0.04701083, 0.033308774, 0.050945673, 0.050938237, 0.04543139, 0.048238277, 0.06305459, 0.052449696]\n",
      "2019-10-25 22:07:48,987 : INFO : topic #3 (0.033): 0.013*\"tree\" + 0.011*\"cell\" + 0.009*\"orientation\" + 0.007*\"map\" + 0.005*\"image\" + 0.005*\"region\" + 0.005*\"field\" + 0.004*\"node\" + 0.004*\"class\" + 0.004*\"center\"\n",
      "2019-10-25 22:07:48,988 : INFO : topic #0 (0.042): 0.009*\"layer\" + 0.008*\"cell\" + 0.008*\"threshold\" + 0.007*\"neuron\" + 0.006*\"node\" + 0.005*\"net\" + 0.005*\"connection\" + 0.004*\"hidden\" + 0.004*\"circuit\" + 0.004*\"activation\"\n",
      "2019-10-25 22:07:48,990 : INFO : topic #4 (0.051): 0.016*\"neuron\" + 0.009*\"cell\" + 0.008*\"response\" + 0.008*\"stimulus\" + 0.007*\"spike\" + 0.006*\"signal\" + 0.006*\"activity\" + 0.005*\"frequency\" + 0.005*\"noise\" + 0.005*\"synaptic\"\n",
      "2019-10-25 22:07:48,992 : INFO : topic #9 (0.052): 0.007*\"classifier\" + 0.007*\"class\" + 0.005*\"classification\" + 0.004*\"node\" + 0.004*\"matrix\" + 0.003*\"sequence\" + 0.003*\"sample\" + 0.003*\"memory\" + 0.003*\"distance\" + 0.003*\"rbf\"\n",
      "2019-10-25 22:07:48,994 : INFO : topic #8 (0.063): 0.005*\"optimal\" + 0.004*\"action\" + 0.004*\"approximation\" + 0.004*\"bound\" + 0.004*\"control\" + 0.004*\"estimate\" + 0.004*\"policy\" + 0.003*\"class\" + 0.003*\"prediction\" + 0.003*\"sample\"\n",
      "2019-10-25 22:07:48,996 : INFO : topic diff=0.209394, rho=0.408248\n",
      "2019-10-25 22:07:49,011 : INFO : PROGRESS: pass 5, at document #1740/1740\n",
      "2019-10-25 22:07:55,952 : INFO : optimized alpha [0.039968695, 0.045309357, 0.04438118, 0.032065723, 0.048853245, 0.05028418, 0.044187408, 0.04691002, 0.062421054, 0.04993263]\n",
      "2019-10-25 22:07:55,965 : INFO : topic #3 (0.032): 0.014*\"tree\" + 0.011*\"cell\" + 0.010*\"orientation\" + 0.008*\"map\" + 0.006*\"field\" + 0.005*\"region\" + 0.005*\"image\" + 0.005*\"receptive\" + 0.004*\"node\" + 0.004*\"center\"\n",
      "2019-10-25 22:07:55,966 : INFO : topic #0 (0.040): 0.009*\"layer\" + 0.008*\"threshold\" + 0.007*\"neuron\" + 0.007*\"cell\" + 0.007*\"node\" + 0.006*\"net\" + 0.005*\"connection\" + 0.004*\"circuit\" + 0.004*\"hidden\" + 0.004*\"activation\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-25 22:07:55,968 : INFO : topic #9 (0.050): 0.008*\"classifier\" + 0.008*\"class\" + 0.006*\"classification\" + 0.005*\"node\" + 0.004*\"matrix\" + 0.003*\"sample\" + 0.003*\"sequence\" + 0.003*\"memory\" + 0.003*\"distance\" + 0.003*\"rbf\"\n",
      "2019-10-25 22:07:55,970 : INFO : topic #5 (0.050): 0.009*\"layer\" + 0.009*\"hidden\" + 0.008*\"recognition\" + 0.007*\"word\" + 0.007*\"speech\" + 0.006*\"net\" + 0.005*\"trained\" + 0.005*\"architecture\" + 0.004*\"hidden_unit\" + 0.004*\"context\"\n",
      "2019-10-25 22:07:55,972 : INFO : topic #8 (0.062): 0.005*\"optimal\" + 0.004*\"action\" + 0.004*\"approximation\" + 0.004*\"bound\" + 0.004*\"estimate\" + 0.004*\"policy\" + 0.004*\"control\" + 0.003*\"prediction\" + 0.003*\"class\" + 0.003*\"sample\"\n",
      "2019-10-25 22:07:55,974 : INFO : topic diff=0.200902, rho=0.377964\n",
      "2019-10-25 22:07:55,988 : INFO : PROGRESS: pass 6, at document #1740/1740\n",
      "2019-10-25 22:08:02,835 : INFO : optimized alpha [0.03868795, 0.04395069, 0.042699773, 0.03115356, 0.047269493, 0.050012153, 0.0433922, 0.04615072, 0.06189665, 0.048249204]\n",
      "2019-10-25 22:08:02,848 : INFO : topic #3 (0.031): 0.015*\"tree\" + 0.012*\"cell\" + 0.010*\"orientation\" + 0.008*\"map\" + 0.006*\"field\" + 0.006*\"region\" + 0.005*\"receptive\" + 0.005*\"image\" + 0.005*\"receptive_field\" + 0.005*\"center\"\n",
      "2019-10-25 22:08:02,850 : INFO : topic #0 (0.039): 0.010*\"layer\" + 0.009*\"threshold\" + 0.008*\"neuron\" + 0.007*\"node\" + 0.006*\"cell\" + 0.006*\"net\" + 0.005*\"connection\" + 0.004*\"circuit\" + 0.004*\"hidden\" + 0.004*\"bit\"\n",
      "2019-10-25 22:08:02,852 : INFO : topic #9 (0.048): 0.009*\"classifier\" + 0.008*\"class\" + 0.006*\"classification\" + 0.005*\"node\" + 0.004*\"sample\" + 0.003*\"matrix\" + 0.003*\"sequence\" + 0.003*\"distance\" + 0.003*\"memory\" + 0.003*\"rbf\"\n",
      "2019-10-25 22:08:02,853 : INFO : topic #5 (0.050): 0.009*\"hidden\" + 0.009*\"layer\" + 0.008*\"recognition\" + 0.007*\"word\" + 0.007*\"speech\" + 0.007*\"net\" + 0.005*\"trained\" + 0.005*\"architecture\" + 0.005*\"hidden_unit\" + 0.004*\"context\"\n",
      "2019-10-25 22:08:02,855 : INFO : topic #8 (0.062): 0.005*\"optimal\" + 0.005*\"action\" + 0.004*\"approximation\" + 0.004*\"bound\" + 0.004*\"policy\" + 0.004*\"estimate\" + 0.004*\"control\" + 0.004*\"prediction\" + 0.003*\"sample\" + 0.003*\"class\"\n",
      "2019-10-25 22:08:02,856 : INFO : topic diff=0.193474, rho=0.353553\n",
      "2019-10-25 22:08:02,871 : INFO : PROGRESS: pass 7, at document #1740/1740\n",
      "2019-10-25 22:08:09,462 : INFO : optimized alpha [0.03786408, 0.043158446, 0.04162309, 0.03045826, 0.046101723, 0.050146654, 0.042845663, 0.045925476, 0.061451044, 0.047003604]\n",
      "2019-10-25 22:08:09,475 : INFO : topic #3 (0.030): 0.015*\"tree\" + 0.012*\"cell\" + 0.011*\"orientation\" + 0.009*\"map\" + 0.007*\"field\" + 0.006*\"region\" + 0.006*\"receptive\" + 0.005*\"receptive_field\" + 0.005*\"image\" + 0.005*\"center\"\n",
      "2019-10-25 22:08:09,477 : INFO : topic #0 (0.038): 0.010*\"layer\" + 0.009*\"threshold\" + 0.008*\"neuron\" + 0.008*\"node\" + 0.007*\"net\" + 0.005*\"cell\" + 0.005*\"connection\" + 0.005*\"bit\" + 0.004*\"circuit\" + 0.004*\"hidden\"\n",
      "2019-10-25 22:08:09,479 : INFO : topic #9 (0.047): 0.010*\"classifier\" + 0.009*\"class\" + 0.006*\"classification\" + 0.005*\"node\" + 0.004*\"sample\" + 0.003*\"distance\" + 0.003*\"rbf\" + 0.003*\"sequence\" + 0.003*\"memory\" + 0.003*\"matrix\"\n",
      "2019-10-25 22:08:09,481 : INFO : topic #5 (0.050): 0.009*\"hidden\" + 0.009*\"layer\" + 0.009*\"recognition\" + 0.008*\"word\" + 0.007*\"speech\" + 0.007*\"net\" + 0.005*\"trained\" + 0.005*\"hidden_unit\" + 0.005*\"architecture\" + 0.004*\"context\"\n",
      "2019-10-25 22:08:09,482 : INFO : topic #8 (0.061): 0.005*\"optimal\" + 0.005*\"action\" + 0.004*\"policy\" + 0.004*\"approximation\" + 0.004*\"bound\" + 0.004*\"estimate\" + 0.004*\"control\" + 0.004*\"prediction\" + 0.003*\"sample\" + 0.003*\"let\"\n",
      "2019-10-25 22:08:09,484 : INFO : topic diff=0.186135, rho=0.333333\n",
      "2019-10-25 22:08:09,499 : INFO : PROGRESS: pass 8, at document #1740/1740\n",
      "2019-10-25 22:08:15,862 : INFO : optimized alpha [0.037322592, 0.042731497, 0.04104168, 0.029918492, 0.045250285, 0.050493136, 0.042562943, 0.046020836, 0.061002966, 0.046199527]\n",
      "2019-10-25 22:08:15,875 : INFO : topic #3 (0.030): 0.015*\"tree\" + 0.012*\"cell\" + 0.011*\"orientation\" + 0.010*\"map\" + 0.007*\"field\" + 0.006*\"receptive\" + 0.006*\"region\" + 0.006*\"receptive_field\" + 0.005*\"center\" + 0.005*\"image\"\n",
      "2019-10-25 22:08:15,877 : INFO : topic #0 (0.037): 0.010*\"layer\" + 0.009*\"threshold\" + 0.008*\"neuron\" + 0.008*\"node\" + 0.007*\"net\" + 0.005*\"bit\" + 0.005*\"connection\" + 0.005*\"bound\" + 0.004*\"theorem\" + 0.004*\"cell\"\n",
      "2019-10-25 22:08:15,879 : INFO : topic #9 (0.046): 0.011*\"classifier\" + 0.009*\"class\" + 0.007*\"classification\" + 0.005*\"node\" + 0.004*\"sample\" + 0.003*\"rbf\" + 0.003*\"distance\" + 0.003*\"memory\" + 0.003*\"sequence\" + 0.003*\"matrix\"\n",
      "2019-10-25 22:08:15,881 : INFO : topic #5 (0.050): 0.010*\"hidden\" + 0.009*\"layer\" + 0.009*\"recognition\" + 0.008*\"word\" + 0.008*\"speech\" + 0.007*\"net\" + 0.005*\"trained\" + 0.005*\"hidden_unit\" + 0.005*\"architecture\" + 0.004*\"context\"\n",
      "2019-10-25 22:08:15,882 : INFO : topic #8 (0.061): 0.005*\"optimal\" + 0.005*\"action\" + 0.004*\"policy\" + 0.004*\"approximation\" + 0.004*\"bound\" + 0.004*\"estimate\" + 0.004*\"control\" + 0.004*\"prediction\" + 0.003*\"sample\" + 0.003*\"reinforcement\"\n",
      "2019-10-25 22:08:15,884 : INFO : topic diff=0.178550, rho=0.316228\n",
      "2019-10-25 22:08:15,899 : INFO : PROGRESS: pass 9, at document #1740/1740\n",
      "2019-10-25 22:08:21,878 : INFO : optimized alpha [0.03705298, 0.04253567, 0.04079875, 0.029491225, 0.044639237, 0.05096411, 0.04245639, 0.046292968, 0.06056781, 0.04571212]\n",
      "2019-10-25 22:08:21,892 : INFO : topic #3 (0.029): 0.016*\"tree\" + 0.012*\"cell\" + 0.011*\"orientation\" + 0.011*\"map\" + 0.008*\"field\" + 0.007*\"receptive\" + 0.006*\"region\" + 0.006*\"receptive_field\" + 0.005*\"center\" + 0.005*\"node\"\n",
      "2019-10-25 22:08:21,894 : INFO : topic #0 (0.037): 0.010*\"layer\" + 0.009*\"threshold\" + 0.008*\"neuron\" + 0.008*\"node\" + 0.007*\"net\" + 0.005*\"bit\" + 0.005*\"bound\" + 0.005*\"theorem\" + 0.005*\"connection\" + 0.004*\"hidden\"\n",
      "2019-10-25 22:08:21,896 : INFO : topic #7 (0.046): 0.006*\"matrix\" + 0.006*\"solution\" + 0.004*\"generalization\" + 0.004*\"noise\" + 0.004*\"gradient\" + 0.004*\"distance\" + 0.004*\"kernel\" + 0.003*\"optimal\" + 0.003*\"dimensional\" + 0.003*\"transformation\"\n",
      "2019-10-25 22:08:21,898 : INFO : topic #5 (0.051): 0.010*\"hidden\" + 0.009*\"recognition\" + 0.009*\"layer\" + 0.008*\"word\" + 0.008*\"speech\" + 0.007*\"net\" + 0.005*\"trained\" + 0.005*\"hidden_unit\" + 0.005*\"architecture\" + 0.004*\"context\"\n",
      "2019-10-25 22:08:21,900 : INFO : topic #8 (0.061): 0.005*\"action\" + 0.005*\"optimal\" + 0.005*\"policy\" + 0.004*\"bound\" + 0.004*\"approximation\" + 0.004*\"estimate\" + 0.004*\"control\" + 0.004*\"prediction\" + 0.004*\"reinforcement\" + 0.003*\"sample\"\n",
      "2019-10-25 22:08:21,901 : INFO : topic diff=0.170439, rho=0.301511\n",
      "2019-10-25 22:08:21,916 : INFO : PROGRESS: pass 10, at document #1740/1740\n",
      "2019-10-25 22:08:27,863 : INFO : optimized alpha [0.03700576, 0.04250469, 0.04077241, 0.029167391, 0.044263374, 0.051573906, 0.042481992, 0.046741094, 0.060180843, 0.045466162]\n",
      "2019-10-25 22:08:27,877 : INFO : topic #3 (0.029): 0.016*\"tree\" + 0.012*\"cell\" + 0.012*\"orientation\" + 0.011*\"map\" + 0.008*\"field\" + 0.007*\"receptive\" + 0.007*\"receptive_field\" + 0.007*\"region\" + 0.005*\"center\" + 0.005*\"node\"\n",
      "2019-10-25 22:08:27,879 : INFO : topic #0 (0.037): 0.010*\"layer\" + 0.009*\"threshold\" + 0.008*\"neuron\" + 0.008*\"node\" + 0.008*\"net\" + 0.005*\"bit\" + 0.005*\"bound\" + 0.005*\"theorem\" + 0.005*\"connection\" + 0.004*\"hidden\"\n",
      "2019-10-25 22:08:27,880 : INFO : topic #7 (0.047): 0.006*\"matrix\" + 0.006*\"solution\" + 0.004*\"generalization\" + 0.004*\"gradient\" + 0.004*\"noise\" + 0.004*\"distance\" + 0.004*\"kernel\" + 0.003*\"optimal\" + 0.003*\"dimensional\" + 0.003*\"transformation\"\n",
      "2019-10-25 22:08:27,882 : INFO : topic #5 (0.052): 0.010*\"hidden\" + 0.009*\"recognition\" + 0.009*\"layer\" + 0.008*\"word\" + 0.008*\"speech\" + 0.007*\"net\" + 0.006*\"trained\" + 0.005*\"hidden_unit\" + 0.005*\"architecture\" + 0.004*\"context\"\n",
      "2019-10-25 22:08:27,883 : INFO : topic #8 (0.060): 0.006*\"action\" + 0.005*\"optimal\" + 0.005*\"policy\" + 0.004*\"estimate\" + 0.004*\"bound\" + 0.004*\"approximation\" + 0.004*\"control\" + 0.004*\"prediction\" + 0.004*\"reinforcement\" + 0.003*\"sample\"\n",
      "2019-10-25 22:08:27,885 : INFO : topic diff=0.162077, rho=0.288675\n",
      "2019-10-25 22:08:27,900 : INFO : PROGRESS: pass 11, at document #1740/1740\n",
      "2019-10-25 22:08:33,662 : INFO : optimized alpha [0.037105504, 0.04254925, 0.040873077, 0.0289801, 0.044060677, 0.052259095, 0.04261183, 0.047385745, 0.059869036, 0.045400083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-25 22:08:33,675 : INFO : topic #3 (0.029): 0.015*\"tree\" + 0.013*\"cell\" + 0.012*\"map\" + 0.012*\"orientation\" + 0.009*\"field\" + 0.008*\"receptive\" + 0.007*\"receptive_field\" + 0.007*\"region\" + 0.006*\"center\" + 0.005*\"node\"\n",
      "2019-10-25 22:08:33,677 : INFO : topic #0 (0.037): 0.010*\"layer\" + 0.009*\"threshold\" + 0.009*\"node\" + 0.009*\"neuron\" + 0.008*\"net\" + 0.006*\"bit\" + 0.006*\"bound\" + 0.005*\"theorem\" + 0.005*\"connection\" + 0.004*\"hidden\"\n",
      "2019-10-25 22:08:33,679 : INFO : topic #7 (0.047): 0.006*\"matrix\" + 0.006*\"solution\" + 0.005*\"gradient\" + 0.004*\"generalization\" + 0.004*\"noise\" + 0.004*\"distance\" + 0.004*\"kernel\" + 0.003*\"optimal\" + 0.003*\"dimensional\" + 0.003*\"xi\"\n",
      "2019-10-25 22:08:33,680 : INFO : topic #5 (0.052): 0.010*\"hidden\" + 0.009*\"recognition\" + 0.009*\"layer\" + 0.008*\"word\" + 0.008*\"speech\" + 0.007*\"net\" + 0.006*\"trained\" + 0.006*\"hidden_unit\" + 0.005*\"architecture\" + 0.004*\"context\"\n",
      "2019-10-25 22:08:33,682 : INFO : topic #8 (0.060): 0.006*\"action\" + 0.006*\"optimal\" + 0.005*\"policy\" + 0.004*\"estimate\" + 0.004*\"bound\" + 0.004*\"approximation\" + 0.004*\"control\" + 0.004*\"prediction\" + 0.004*\"reinforcement\" + 0.003*\"sample\"\n",
      "2019-10-25 22:08:33,683 : INFO : topic diff=0.153599, rho=0.277350\n",
      "2019-10-25 22:08:33,699 : INFO : PROGRESS: pass 12, at document #1740/1740\n",
      "2019-10-25 22:08:39,507 : INFO : optimized alpha [0.037330467, 0.042656958, 0.041031435, 0.028863886, 0.04393896, 0.0531347, 0.042829942, 0.04815365, 0.05953605, 0.04548313]\n",
      "2019-10-25 22:08:39,520 : INFO : topic #3 (0.029): 0.015*\"tree\" + 0.013*\"cell\" + 0.013*\"map\" + 0.012*\"orientation\" + 0.009*\"field\" + 0.008*\"receptive\" + 0.008*\"receptive_field\" + 0.007*\"region\" + 0.006*\"center\" + 0.005*\"node\"\n",
      "2019-10-25 22:08:39,522 : INFO : topic #0 (0.037): 0.010*\"layer\" + 0.009*\"threshold\" + 0.009*\"node\" + 0.009*\"neuron\" + 0.008*\"net\" + 0.006*\"bound\" + 0.006*\"bit\" + 0.005*\"theorem\" + 0.005*\"connection\" + 0.004*\"class\"\n",
      "2019-10-25 22:08:39,524 : INFO : topic #7 (0.048): 0.006*\"matrix\" + 0.006*\"solution\" + 0.005*\"gradient\" + 0.005*\"generalization\" + 0.004*\"noise\" + 0.004*\"distance\" + 0.004*\"kernel\" + 0.003*\"optimal\" + 0.003*\"dimensional\" + 0.003*\"minimum\"\n",
      "2019-10-25 22:08:39,525 : INFO : topic #5 (0.053): 0.011*\"hidden\" + 0.009*\"recognition\" + 0.009*\"layer\" + 0.008*\"word\" + 0.008*\"speech\" + 0.007*\"net\" + 0.006*\"trained\" + 0.006*\"hidden_unit\" + 0.005*\"architecture\" + 0.004*\"context\"\n",
      "2019-10-25 22:08:39,527 : INFO : topic #8 (0.060): 0.006*\"action\" + 0.006*\"optimal\" + 0.005*\"policy\" + 0.004*\"estimate\" + 0.004*\"bound\" + 0.004*\"approximation\" + 0.004*\"control\" + 0.004*\"reinforcement\" + 0.004*\"prediction\" + 0.003*\"let\"\n",
      "2019-10-25 22:08:39,529 : INFO : topic diff=0.145126, rho=0.267261\n",
      "2019-10-25 22:08:39,543 : INFO : PROGRESS: pass 13, at document #1740/1740\n",
      "2019-10-25 22:08:45,347 : INFO : optimized alpha [0.037597973, 0.042891838, 0.041245252, 0.02883589, 0.043906655, 0.05400896, 0.043117855, 0.04895965, 0.059300985, 0.04568587]\n",
      "2019-10-25 22:08:45,360 : INFO : topic #3 (0.029): 0.015*\"tree\" + 0.013*\"map\" + 0.013*\"cell\" + 0.012*\"orientation\" + 0.010*\"field\" + 0.008*\"receptive\" + 0.008*\"receptive_field\" + 0.007*\"region\" + 0.006*\"center\" + 0.005*\"node\"\n",
      "2019-10-25 22:08:45,362 : INFO : topic #0 (0.038): 0.010*\"layer\" + 0.009*\"threshold\" + 0.009*\"node\" + 0.009*\"neuron\" + 0.008*\"net\" + 0.006*\"bound\" + 0.006*\"bit\" + 0.006*\"theorem\" + 0.004*\"class\" + 0.004*\"connection\"\n",
      "2019-10-25 22:08:45,363 : INFO : topic #7 (0.049): 0.006*\"matrix\" + 0.006*\"solution\" + 0.005*\"gradient\" + 0.005*\"generalization\" + 0.005*\"noise\" + 0.004*\"distance\" + 0.004*\"kernel\" + 0.004*\"optimal\" + 0.003*\"dimensional\" + 0.003*\"minimum\"\n",
      "2019-10-25 22:08:45,365 : INFO : topic #5 (0.054): 0.011*\"hidden\" + 0.009*\"recognition\" + 0.009*\"layer\" + 0.008*\"word\" + 0.008*\"speech\" + 0.008*\"net\" + 0.006*\"trained\" + 0.006*\"hidden_unit\" + 0.005*\"architecture\" + 0.004*\"sequence\"\n",
      "2019-10-25 22:08:45,367 : INFO : topic #8 (0.059): 0.006*\"action\" + 0.006*\"optimal\" + 0.005*\"policy\" + 0.004*\"estimate\" + 0.004*\"reinforcement\" + 0.004*\"bound\" + 0.004*\"control\" + 0.004*\"approximation\" + 0.004*\"prediction\" + 0.003*\"let\"\n",
      "2019-10-25 22:08:45,369 : INFO : topic diff=0.136836, rho=0.258199\n",
      "2019-10-25 22:08:45,384 : INFO : PROGRESS: pass 14, at document #1740/1740\n",
      "2019-10-25 22:08:50,999 : INFO : optimized alpha [0.037912805, 0.043174498, 0.04153439, 0.028857369, 0.04392821, 0.054896977, 0.04344708, 0.049807522, 0.059066627, 0.045954]\n",
      "2019-10-25 22:08:51,013 : INFO : topic #3 (0.029): 0.015*\"tree\" + 0.014*\"map\" + 0.013*\"cell\" + 0.012*\"orientation\" + 0.010*\"field\" + 0.009*\"receptive\" + 0.008*\"receptive_field\" + 0.007*\"region\" + 0.006*\"center\" + 0.005*\"self\"\n",
      "2019-10-25 22:08:51,015 : INFO : topic #0 (0.038): 0.010*\"layer\" + 0.009*\"threshold\" + 0.009*\"node\" + 0.009*\"neuron\" + 0.008*\"net\" + 0.006*\"bound\" + 0.006*\"bit\" + 0.006*\"theorem\" + 0.005*\"class\" + 0.005*\"let\"\n",
      "2019-10-25 22:08:51,017 : INFO : topic #7 (0.050): 0.006*\"matrix\" + 0.006*\"solution\" + 0.005*\"gradient\" + 0.005*\"generalization\" + 0.005*\"noise\" + 0.004*\"distance\" + 0.004*\"kernel\" + 0.004*\"optimal\" + 0.004*\"minimum\" + 0.003*\"dimensional\"\n",
      "2019-10-25 22:08:51,018 : INFO : topic #5 (0.055): 0.011*\"hidden\" + 0.009*\"recognition\" + 0.009*\"layer\" + 0.008*\"speech\" + 0.008*\"word\" + 0.008*\"net\" + 0.006*\"trained\" + 0.006*\"hidden_unit\" + 0.005*\"architecture\" + 0.005*\"sequence\"\n",
      "2019-10-25 22:08:51,020 : INFO : topic #8 (0.059): 0.006*\"action\" + 0.006*\"optimal\" + 0.005*\"policy\" + 0.004*\"reinforcement\" + 0.004*\"estimate\" + 0.004*\"control\" + 0.004*\"prediction\" + 0.004*\"bound\" + 0.004*\"approximation\" + 0.003*\"let\"\n",
      "2019-10-25 22:08:51,022 : INFO : topic diff=0.128856, rho=0.250000\n",
      "2019-10-25 22:08:51,037 : INFO : PROGRESS: pass 15, at document #1740/1740\n",
      "2019-10-25 22:08:56,540 : INFO : optimized alpha [0.038228594, 0.043498006, 0.041849963, 0.028898777, 0.043981284, 0.055764575, 0.043776967, 0.05070219, 0.058890507, 0.046248667]\n",
      "2019-10-25 22:08:56,553 : INFO : topic #3 (0.029): 0.014*\"tree\" + 0.014*\"map\" + 0.014*\"cell\" + 0.012*\"orientation\" + 0.011*\"field\" + 0.009*\"receptive\" + 0.009*\"receptive_field\" + 0.007*\"region\" + 0.006*\"center\" + 0.005*\"self\"\n",
      "2019-10-25 22:08:56,555 : INFO : topic #0 (0.038): 0.010*\"layer\" + 0.009*\"node\" + 0.009*\"threshold\" + 0.009*\"neuron\" + 0.008*\"net\" + 0.007*\"bound\" + 0.006*\"bit\" + 0.006*\"theorem\" + 0.005*\"let\" + 0.005*\"class\"\n",
      "2019-10-25 22:08:56,557 : INFO : topic #7 (0.051): 0.006*\"matrix\" + 0.006*\"solution\" + 0.005*\"gradient\" + 0.005*\"generalization\" + 0.005*\"noise\" + 0.004*\"kernel\" + 0.004*\"distance\" + 0.004*\"minimum\" + 0.004*\"optimal\" + 0.003*\"dimensional\"\n",
      "2019-10-25 22:08:56,558 : INFO : topic #5 (0.056): 0.011*\"hidden\" + 0.009*\"recognition\" + 0.009*\"layer\" + 0.008*\"speech\" + 0.008*\"word\" + 0.008*\"net\" + 0.006*\"trained\" + 0.006*\"hidden_unit\" + 0.006*\"architecture\" + 0.005*\"sequence\"\n",
      "2019-10-25 22:08:56,560 : INFO : topic #8 (0.059): 0.007*\"action\" + 0.006*\"optimal\" + 0.006*\"policy\" + 0.005*\"reinforcement\" + 0.004*\"control\" + 0.004*\"estimate\" + 0.004*\"prediction\" + 0.004*\"bound\" + 0.004*\"approximation\" + 0.003*\"let\"\n",
      "2019-10-25 22:08:56,562 : INFO : topic diff=0.121244, rho=0.242536\n",
      "2019-10-25 22:08:56,577 : INFO : PROGRESS: pass 16, at document #1740/1740\n",
      "2019-10-25 22:09:01,924 : INFO : optimized alpha [0.038598184, 0.04383293, 0.042222213, 0.028939702, 0.044080146, 0.056641337, 0.044149432, 0.05160987, 0.058673337, 0.046631392]\n",
      "2019-10-25 22:09:01,937 : INFO : topic #3 (0.029): 0.014*\"map\" + 0.014*\"tree\" + 0.014*\"cell\" + 0.013*\"orientation\" + 0.011*\"field\" + 0.009*\"receptive\" + 0.009*\"receptive_field\" + 0.007*\"region\" + 0.006*\"center\" + 0.005*\"self\"\n",
      "2019-10-25 22:09:01,939 : INFO : topic #0 (0.039): 0.010*\"layer\" + 0.009*\"node\" + 0.009*\"threshold\" + 0.009*\"neuron\" + 0.008*\"net\" + 0.007*\"bound\" + 0.006*\"bit\" + 0.006*\"theorem\" + 0.005*\"let\" + 0.005*\"class\"\n",
      "2019-10-25 22:09:01,941 : INFO : topic #7 (0.052): 0.007*\"matrix\" + 0.006*\"solution\" + 0.005*\"gradient\" + 0.005*\"generalization\" + 0.005*\"noise\" + 0.004*\"kernel\" + 0.004*\"minimum\" + 0.004*\"optimal\" + 0.004*\"distance\" + 0.003*\"dimensional\"\n",
      "2019-10-25 22:09:01,942 : INFO : topic #5 (0.057): 0.011*\"hidden\" + 0.009*\"recognition\" + 0.009*\"layer\" + 0.008*\"speech\" + 0.008*\"word\" + 0.008*\"net\" + 0.006*\"trained\" + 0.006*\"hidden_unit\" + 0.006*\"architecture\" + 0.005*\"sequence\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-25 22:09:01,944 : INFO : topic #8 (0.059): 0.007*\"action\" + 0.006*\"optimal\" + 0.006*\"policy\" + 0.005*\"reinforcement\" + 0.004*\"control\" + 0.004*\"prediction\" + 0.004*\"estimate\" + 0.004*\"approximation\" + 0.004*\"bound\" + 0.003*\"let\"\n",
      "2019-10-25 22:09:01,945 : INFO : topic diff=0.114043, rho=0.235702\n",
      "2019-10-25 22:09:01,960 : INFO : PROGRESS: pass 17, at document #1740/1740\n",
      "2019-10-25 22:09:07,350 : INFO : optimized alpha [0.039000735, 0.044152234, 0.042632557, 0.029003998, 0.044210915, 0.057528045, 0.044507593, 0.05254776, 0.05852229, 0.047058154]\n",
      "2019-10-25 22:09:07,364 : INFO : topic #3 (0.029): 0.015*\"map\" + 0.014*\"cell\" + 0.014*\"tree\" + 0.013*\"orientation\" + 0.011*\"field\" + 0.010*\"receptive\" + 0.009*\"receptive_field\" + 0.008*\"region\" + 0.006*\"center\" + 0.005*\"self\"\n",
      "2019-10-25 22:09:07,366 : INFO : topic #0 (0.039): 0.010*\"layer\" + 0.009*\"node\" + 0.009*\"threshold\" + 0.009*\"neuron\" + 0.008*\"net\" + 0.007*\"bound\" + 0.006*\"bit\" + 0.006*\"theorem\" + 0.005*\"let\" + 0.005*\"class\"\n",
      "2019-10-25 22:09:07,368 : INFO : topic #7 (0.053): 0.007*\"matrix\" + 0.006*\"solution\" + 0.005*\"gradient\" + 0.005*\"generalization\" + 0.005*\"noise\" + 0.004*\"minimum\" + 0.004*\"kernel\" + 0.004*\"optimal\" + 0.004*\"distance\" + 0.003*\"dimensional\"\n",
      "2019-10-25 22:09:07,369 : INFO : topic #5 (0.058): 0.011*\"hidden\" + 0.009*\"recognition\" + 0.009*\"layer\" + 0.008*\"speech\" + 0.008*\"word\" + 0.008*\"net\" + 0.006*\"hidden_unit\" + 0.006*\"trained\" + 0.006*\"architecture\" + 0.005*\"sequence\"\n",
      "2019-10-25 22:09:07,371 : INFO : topic #8 (0.059): 0.007*\"action\" + 0.006*\"optimal\" + 0.006*\"policy\" + 0.005*\"reinforcement\" + 0.004*\"control\" + 0.004*\"prediction\" + 0.004*\"estimate\" + 0.004*\"approximation\" + 0.004*\"bound\" + 0.003*\"let\"\n",
      "2019-10-25 22:09:07,372 : INFO : topic diff=0.107323, rho=0.229416\n",
      "2019-10-25 22:09:07,387 : INFO : PROGRESS: pass 18, at document #1740/1740\n",
      "2019-10-25 22:09:12,636 : INFO : optimized alpha [0.03946321, 0.04451048, 0.043087192, 0.02908224, 0.044362865, 0.058412015, 0.044875894, 0.053533968, 0.058476444, 0.04753619]\n",
      "2019-10-25 22:09:12,650 : INFO : topic #3 (0.029): 0.015*\"map\" + 0.014*\"cell\" + 0.013*\"tree\" + 0.013*\"orientation\" + 0.012*\"field\" + 0.010*\"receptive\" + 0.009*\"receptive_field\" + 0.008*\"region\" + 0.006*\"center\" + 0.006*\"self\"\n",
      "2019-10-25 22:09:12,651 : INFO : topic #0 (0.039): 0.010*\"layer\" + 0.009*\"node\" + 0.009*\"threshold\" + 0.009*\"neuron\" + 0.008*\"net\" + 0.008*\"bound\" + 0.006*\"bit\" + 0.006*\"theorem\" + 0.005*\"let\" + 0.005*\"class\"\n",
      "2019-10-25 22:09:12,653 : INFO : topic #7 (0.054): 0.007*\"matrix\" + 0.006*\"solution\" + 0.005*\"gradient\" + 0.005*\"generalization\" + 0.005*\"noise\" + 0.004*\"minimum\" + 0.004*\"kernel\" + 0.004*\"optimal\" + 0.004*\"dimensional\" + 0.003*\"distance\"\n",
      "2019-10-25 22:09:12,655 : INFO : topic #5 (0.058): 0.011*\"hidden\" + 0.009*\"recognition\" + 0.009*\"layer\" + 0.008*\"speech\" + 0.008*\"word\" + 0.008*\"net\" + 0.006*\"hidden_unit\" + 0.006*\"trained\" + 0.006*\"architecture\" + 0.005*\"sequence\"\n",
      "2019-10-25 22:09:12,656 : INFO : topic #8 (0.058): 0.007*\"action\" + 0.006*\"optimal\" + 0.006*\"policy\" + 0.005*\"reinforcement\" + 0.005*\"control\" + 0.004*\"prediction\" + 0.004*\"estimate\" + 0.004*\"approximation\" + 0.004*\"bound\" + 0.003*\"let\"\n",
      "2019-10-25 22:09:12,658 : INFO : topic diff=0.101076, rho=0.223607\n",
      "2019-10-25 22:09:12,673 : INFO : PROGRESS: pass 19, at document #1740/1740\n",
      "2019-10-25 22:09:18,098 : INFO : optimized alpha [0.03998699, 0.044886053, 0.043563645, 0.02920096, 0.04452222, 0.05931185, 0.045296088, 0.05455838, 0.058434974, 0.048047427]\n",
      "2019-10-25 22:09:18,113 : INFO : topic #3 (0.029): 0.015*\"map\" + 0.015*\"cell\" + 0.013*\"orientation\" + 0.012*\"tree\" + 0.012*\"field\" + 0.010*\"receptive\" + 0.010*\"receptive_field\" + 0.008*\"region\" + 0.006*\"center\" + 0.006*\"self\"\n",
      "2019-10-25 22:09:18,115 : INFO : topic #0 (0.040): 0.010*\"layer\" + 0.009*\"node\" + 0.009*\"threshold\" + 0.009*\"neuron\" + 0.008*\"net\" + 0.008*\"bound\" + 0.006*\"bit\" + 0.006*\"theorem\" + 0.005*\"let\" + 0.005*\"class\"\n",
      "2019-10-25 22:09:18,116 : INFO : topic #7 (0.055): 0.007*\"matrix\" + 0.006*\"solution\" + 0.005*\"gradient\" + 0.005*\"generalization\" + 0.005*\"noise\" + 0.004*\"minimum\" + 0.004*\"kernel\" + 0.004*\"optimal\" + 0.004*\"dimensional\" + 0.003*\"distance\"\n",
      "2019-10-25 22:09:18,118 : INFO : topic #8 (0.058): 0.007*\"action\" + 0.006*\"optimal\" + 0.006*\"policy\" + 0.005*\"reinforcement\" + 0.005*\"control\" + 0.004*\"prediction\" + 0.004*\"estimate\" + 0.004*\"approximation\" + 0.004*\"bound\" + 0.003*\"let\"\n",
      "2019-10-25 22:09:18,120 : INFO : topic #5 (0.059): 0.012*\"hidden\" + 0.009*\"recognition\" + 0.009*\"layer\" + 0.008*\"speech\" + 0.008*\"word\" + 0.008*\"net\" + 0.006*\"hidden_unit\" + 0.006*\"trained\" + 0.006*\"architecture\" + 0.005*\"sequence\"\n",
      "2019-10-25 22:09:18,121 : INFO : topic diff=0.095266, rho=0.218218\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the topic coherence of each topic. Below we display the\n",
    "average topic coherence and print the topics in order of topic coherence.\n",
    "\n",
    "Note that we use the \"Umass\" topic coherence measure here (see\n",
    ":py:func:`gensim.models.ldamodel.LdaModel.top_topics`), Gensim has recently\n",
    "obtained an implementation of the \"AKSW\" topic coherence measure (see\n",
    "accompanying blog post, http://rare-technologies.com/what-is-topic-coherence/).\n",
    "\n",
    "If you are familiar with the subject of the articles in this dataset, you can\n",
    "see that the topics below make a lot of sense. However, they are not without\n",
    "flaws. We can see that there is substantial overlap between some topics,\n",
    "others are hard to interpret, and most of them have at least some terms that\n",
    "seem out of place. If you were able to do better, feel free to share your\n",
    "methods on the blog at http://rare-technologies.com/lda-training-tips/ !\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-25 22:09:18,298 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.2219.\n",
      "[([(0.009883207, u'gaussian'),\n",
      "   (0.009447834, u'mixture'),\n",
      "   (0.007822083, u'likelihood'),\n",
      "   (0.007776968, u'density'),\n",
      "   (0.0069660144, u'component'),\n",
      "   (0.0067372774, u'prior'),\n",
      "   (0.006166739, u'log'),\n",
      "   (0.0057867425, u'matrix'),\n",
      "   (0.005542608, u'bayesian'),\n",
      "   (0.0052381656, u'rule'),\n",
      "   (0.005175795, u'posterior'),\n",
      "   (0.004856301, u'hidden'),\n",
      "   (0.0045446404, u'estimate'),\n",
      "   (0.0043667345, u'noise'),\n",
      "   (0.0043658726, u'em'),\n",
      "   (0.0042184903, u'source'),\n",
      "   (0.00402033, u'independent'),\n",
      "   (0.0038268669, u'estimation'),\n",
      "   (0.0036593245, u'maximum'),\n",
      "   (0.0036540474, u'approximation')],\n",
      "  -0.8464928153187588),\n",
      " ([(0.018307831, u'neuron'),\n",
      "   (0.016192446, u'cell'),\n",
      "   (0.010200098, u'response'),\n",
      "   (0.009976928, u'stimulus'),\n",
      "   (0.008517418, u'spike'),\n",
      "   (0.008286243, u'activity'),\n",
      "   (0.006442301, u'synaptic'),\n",
      "   (0.0062103174, u'firing'),\n",
      "   (0.006186208, u'signal'),\n",
      "   (0.006078393, u'frequency'),\n",
      "   (0.0049683326, u'visual'),\n",
      "   (0.004797134, u'cortex'),\n",
      "   (0.004255529, u'noise'),\n",
      "   (0.003862201, u'temporal'),\n",
      "   (0.0038597714, u'connection'),\n",
      "   (0.0038574908, u'channel'),\n",
      "   (0.0038237146, u'potential'),\n",
      "   (0.0036902647, u'direction'),\n",
      "   (0.003619714, u'cortical'),\n",
      "   (0.0032409693, u'excitatory')],\n",
      "  -0.9789821357797283),\n",
      " ([(0.00724977, u'action'),\n",
      "   (0.006449727, u'optimal'),\n",
      "   (0.0060048876, u'policy'),\n",
      "   (0.0049770065, u'reinforcement'),\n",
      "   (0.004655333, u'control'),\n",
      "   (0.0043722023, u'prediction'),\n",
      "   (0.0042025954, u'estimate'),\n",
      "   (0.0039298427, u'approximation'),\n",
      "   (0.0037414667, u'bound'),\n",
      "   (0.0034248475, u'let'),\n",
      "   (0.0034068427, u'reinforcement_learning'),\n",
      "   (0.00322228, u'convergence'),\n",
      "   (0.0031438048, u'sample'),\n",
      "   (0.0030998075, u'decision'),\n",
      "   (0.0030969384, u'loss'),\n",
      "   (0.0030040117, u'cost'),\n",
      "   (0.0029215333, u'stochastic'),\n",
      "   (0.00275896, u'reward'),\n",
      "   (0.0026588328, u'dynamic'),\n",
      "   (0.0026178365, u'expected')],\n",
      "  -1.087801659711457),\n",
      " ([(0.030604798, u'image'),\n",
      "   (0.014480195, u'object'),\n",
      "   (0.008808374, u'visual'),\n",
      "   (0.007987093, u'motion'),\n",
      "   (0.0068788636, u'pixel'),\n",
      "   (0.0059003583, u'face'),\n",
      "   (0.005694975, u'recognition'),\n",
      "   (0.005168649, u'position'),\n",
      "   (0.0050307526, u'field'),\n",
      "   (0.0047504506, u'view'),\n",
      "   (0.0046209088, u'location'),\n",
      "   (0.0044446345, u'filter'),\n",
      "   (0.004427778, u'map'),\n",
      "   (0.004295089, u'vision'),\n",
      "   (0.004184722, u'direction'),\n",
      "   (0.003944283, u'eye'),\n",
      "   (0.0035086232, u'signal'),\n",
      "   (0.0033725663, u'target'),\n",
      "   (0.0033713656, u'scene'),\n",
      "   (0.003249951, u'edge')],\n",
      "  -1.1127728058742636),\n",
      " ([(0.00669199, u'matrix'),\n",
      "   (0.006411751, u'solution'),\n",
      "   (0.005425709, u'gradient'),\n",
      "   (0.0050941687, u'generalization'),\n",
      "   (0.0048882416, u'noise'),\n",
      "   (0.003818124, u'minimum'),\n",
      "   (0.003707605, u'kernel'),\n",
      "   (0.003706757, u'optimal'),\n",
      "   (0.0035147578, u'dimensional'),\n",
      "   (0.0034161368, u'distance'),\n",
      "   (0.00334086, u'constraint'),\n",
      "   (0.0033298475, u'xi'),\n",
      "   (0.003200136, u'descent'),\n",
      "   (0.003167068, u'optimization'),\n",
      "   (0.0029712617, u'transformation'),\n",
      "   (0.002908996, u'convergence'),\n",
      "   (0.0029065711, u'training_set'),\n",
      "   (0.0029011276, u'dynamic'),\n",
      "   (0.002746394, u'eq'),\n",
      "   (0.0027300525, u'nonlinear')],\n",
      "  -1.137785382933355),\n",
      " ([(0.011520438, u'hidden'),\n",
      "   (0.009076586, u'recognition'),\n",
      "   (0.008917178, u'layer'),\n",
      "   (0.008147796, u'speech'),\n",
      "   (0.008023689, u'word'),\n",
      "   (0.007826777, u'net'),\n",
      "   (0.006079669, u'hidden_unit'),\n",
      "   (0.0060228542, u'trained'),\n",
      "   (0.0056677097, u'architecture'),\n",
      "   (0.00483744, u'sequence'),\n",
      "   (0.004539834, u'context'),\n",
      "   (0.004259585, u'rule'),\n",
      "   (0.003894094, u'recurrent'),\n",
      "   (0.003701296, u'character'),\n",
      "   (0.0036953073, u'back'),\n",
      "   (0.0034523567, u'activation'),\n",
      "   (0.0033013558, u'connectionist'),\n",
      "   (0.0032779635, u'propagation'),\n",
      "   (0.003152254, u'language'),\n",
      "   (0.0030964077, u'speaker')],\n",
      "  -1.1877852444202948),\n",
      " ([(0.012852228, u'classifier'),\n",
      "   (0.011781849, u'class'),\n",
      "   (0.009650117, u'classification'),\n",
      "   (0.0057012, u'node'),\n",
      "   (0.0050326493, u'sample'),\n",
      "   (0.004267119, u'distance'),\n",
      "   (0.004141899, u'rbf'),\n",
      "   (0.0039838115, u'decision'),\n",
      "   (0.0039607733, u'tree'),\n",
      "   (0.0039095883, u'cluster'),\n",
      "   (0.0033099018, u'clustering'),\n",
      "   (0.0032266553, u'nearest'),\n",
      "   (0.0031629582, u'training_set'),\n",
      "   (0.0029985004, u'graph'),\n",
      "   (0.0029940035, u'neighbor'),\n",
      "   (0.0029858064, u'table'),\n",
      "   (0.0028358197, u'search'),\n",
      "   (0.002812526, u'memory'),\n",
      "   (0.0027090013, u'prediction'),\n",
      "   (0.002701442, u'basis')],\n",
      "  -1.2549438193822327),\n",
      " ([(0.009537477, u'layer'),\n",
      "   (0.0094393175, u'node'),\n",
      "   (0.008844939, u'threshold'),\n",
      "   (0.008610412, u'neuron'),\n",
      "   (0.008428063, u'net'),\n",
      "   (0.007776696, u'bound'),\n",
      "   (0.0063522602, u'bit'),\n",
      "   (0.0063277367, u'theorem'),\n",
      "   (0.0053407983, u'let'),\n",
      "   (0.005013048, u'class'),\n",
      "   (0.004710574, u'proof'),\n",
      "   (0.004304556, u'polynomial'),\n",
      "   (0.004177932, u'hidden'),\n",
      "   (0.0041701635, u'processor'),\n",
      "   (0.0041117044, u'connection'),\n",
      "   (0.0040489347, u'activation'),\n",
      "   (0.004038177, u'dimension'),\n",
      "   (0.0037972901, u'gate'),\n",
      "   (0.0035274886, u'parallel'),\n",
      "   (0.0034787483, u'element')],\n",
      "  -1.2610087132056496),\n",
      " ([(0.011600035, u'neuron'),\n",
      "   (0.010636022, u'control'),\n",
      "   (0.009687162, u'memory'),\n",
      "   (0.009536286, u'circuit'),\n",
      "   (0.0073195035, u'dynamic'),\n",
      "   (0.005499723, u'chip'),\n",
      "   (0.005406787, u'signal'),\n",
      "   (0.005370661, u'voltage'),\n",
      "   (0.005327675, u'analog'),\n",
      "   (0.005247753, u'motor'),\n",
      "   (0.0046887333, u'trajectory'),\n",
      "   (0.0043627624, u'feedback'),\n",
      "   (0.004274461, u'fig'),\n",
      "   (0.003875611, u'delay'),\n",
      "   (0.0037524512, u'movement'),\n",
      "   (0.0035565197, u'controller'),\n",
      "   (0.0033801633, u'behavior'),\n",
      "   (0.0032258052, u'connection'),\n",
      "   (0.003133521, u'pulse'),\n",
      "   (0.0031181928, u'associative')],\n",
      "  -1.4225085347126574),\n",
      " ([(0.015461903, u'map'),\n",
      "   (0.014628406, u'cell'),\n",
      "   (0.012839555, u'orientation'),\n",
      "   (0.012483464, u'tree'),\n",
      "   (0.012018518, u'field'),\n",
      "   (0.010155119, u'receptive'),\n",
      "   (0.009564235, u'receptive_field'),\n",
      "   (0.007672509, u'region'),\n",
      "   (0.00621342, u'center'),\n",
      "   (0.005672087, u'self'),\n",
      "   (0.004545611, u'dominance'),\n",
      "   (0.0042508855, u'ocular'),\n",
      "   (0.004126568, u'node'),\n",
      "   (0.004116698, u'fig'),\n",
      "   (0.00403268, u'ocular_dominance'),\n",
      "   (0.0040023434, u'position'),\n",
      "   (0.003881942, u'cortex'),\n",
      "   (0.0036556085, u'layer'),\n",
      "   (0.0035227027, u'wavelet'),\n",
      "   (0.0035163844, u'visual')],\n",
      "  -1.9292657284093468)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to experiment with\n",
    "-------------------------\n",
    "\n",
    "* ``no_above`` and ``no_below`` parameters in ``filter_extremes`` method.\n",
    "* Adding trigrams or even higher order n-grams.\n",
    "* Consider whether using a hold-out set or cross-validation is the way to go for you.\n",
    "* Try other datasets.\n",
    "\n",
    "Where to go from here\n",
    "---------------------\n",
    "\n",
    "* Check out a RaRe blog post on the AKSW topic coherence measure (http://rare-technologies.com/what-is-topic-coherence/).\n",
    "* pyLDAvis (https://pyldavis.readthedocs.io/en/latest/index.html).\n",
    "* Read some more Gensim tutorials (https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials).\n",
    "* If you haven't already, read [1] and [2] (see references).\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "1. \"Latent Dirichlet Allocation\", Blei et al. 2003.\n",
    "2. \"Online Learning for Latent Dirichlet Allocation\", Hoffman et al. 2010.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
